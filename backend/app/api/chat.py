"""
AIèŠå¤©API / AI chat API
ç»Ÿä¸€çš„æ™ºèƒ½èŠå¤©æ¥å£ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨ / Unified intelligent chat interface with tool calling
"""

from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
from typing import Optional, Dict, Any
import logging
import json
import uuid
import time

from ..models.user import ChatRequest, ChatResponse
from ..services.claude_service import ClaudeService
from ..core.logging import log_api_access, log_performance, log_error_with_context, get_logger

logger = get_logger("JobCatcher.api.chat")
router = APIRouter()

# å…¨å±€ClaudeServiceå®ä¾‹ï¼Œä¿æŒä¼šè¯çŠ¶æ€ / Global ClaudeService instance to maintain session state
_global_claude_service = None

def get_claude_service() -> ClaudeService:
    """è·å–å…¨å±€ClaudeServiceå®ä¾‹ / Get global ClaudeService instance"""
    global _global_claude_service
    if _global_claude_service is None:
        _global_claude_service = ClaudeService()
        logger.info("Created global ClaudeService instance for session management")
    return _global_claude_service

def _should_trigger_job_query(message: str, context: Dict[Any, Any]) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘èŒä½æŸ¥è¯¢ / Determine if job query should be triggered
    ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸¥æ ¼é™åˆ¶åªåœ¨ç®€å†ä¸Šä¼ åæˆ–æ˜ç¡®èŒä½è¯·æ±‚æ—¶è§¦å‘ / CRITICAL FIX: Strictly limit to resume upload or explicit job requests
    """
    message_lower = message.lower()
    
    # ğŸ”¥ é¦–è¦æ£€æŸ¥ï¼šæ˜¯å¦æ˜¯æŠ€èƒ½çƒ­ç‚¹å›¾è¯·æ±‚ï¼Œå¦‚æœæ˜¯åˆ™ç›´æ¥ä¸è§¦å‘èŒä½æŸ¥è¯¢ / PRIMARY CHECK: If skill heatmap request, never trigger job query
    skill_heatmap_keywords = [
        'skills heatmap', 'skill heatmap', 'heatmap', 'æŠ€èƒ½çƒ­ç‚¹å›¾', 'æŠ€èƒ½çƒ­åŠ›å›¾', 
        'skills analysis', 'skill analysis', 'æŠ€èƒ½åˆ†æ', 'trending skills', 'çƒ­é—¨æŠ€èƒ½',
        'skill map', 'æŠ€èƒ½åœ°å›¾', 'èƒ½åŠ›å›¾è°±', 'skill trends', 'æŠ€èƒ½è¶‹åŠ¿'
    ]
    
    is_skill_heatmap_request = any(keyword in message_lower for keyword in skill_heatmap_keywords)
    if is_skill_heatmap_request:
        logger.info(f"ğŸš« Job query NOT triggered: Skill heatmap request detected - '{message[:50]}...'")
        return False
    
    # ğŸ”¥ æ ¸å¿ƒé€»è¾‘1ï¼šåªæœ‰åœ¨ä¸Šä¼ ç®€å†çš„æƒ…å†µä¸‹ï¼Œæ‰è€ƒè™‘èŒä½æ¨è / CORE LOGIC 1: Only consider job recommendations when resume is uploaded
    has_resume = context.get('resume_uploaded') or context.get('uploaded_file')
    
    if not has_resume:
        # ğŸ”¥ ä¸¥æ ¼é™åˆ¶ï¼šæ²¡æœ‰ç®€å†æ—¶ï¼Œåªåœ¨æ˜ç¡®çš„èŒä½æœç´¢è¯·æ±‚ä¸‹æ‰è§¦å‘ / STRICT LIMIT: Without resume, only trigger for explicit job search
        explicit_job_search_keywords = [
            'find job', 'search job', 'job search', 'job listing', 'job posting',
            'æ‰¾å·¥ä½œ', 'æœç´¢å·¥ä½œ', 'èŒä½æœç´¢', 'æ‹›è˜ä¿¡æ¯', 'å·¥ä½œæœºä¼š'
        ]
        
        is_explicit_job_search = any(keyword in message_lower for keyword in explicit_job_search_keywords)
        if is_explicit_job_search:
            logger.info(f"ğŸ” Job query triggered: Explicit job search without resume - '{message[:50]}...'")
            return True
        else:
            logger.info(f"ğŸš« Job query NOT triggered: No resume uploaded and not explicit job search - '{message[:50]}...'")
            return False
    
    # ğŸ”¥ æ ¸å¿ƒé€»è¾‘2ï¼šæœ‰ç®€å†çš„æƒ…å†µä¸‹ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦èŒä½æ¨è / CORE LOGIC 2: With resume, check if job recommendation needed
    
    # 1. åˆšä¸Šä¼ ç®€å†æ—¶è‡ªåŠ¨è§¦å‘ / Auto-trigger when resume just uploaded
    if context.get('resume_uploaded'):
        logger.info("ğŸ” Job query triggered: Resume just uploaded")
        return True
    
    # 2. ç”¨æˆ·æ˜ç¡®è¦æ±‚èŒä½æ¨è / User explicitly requests job recommendations
    job_request_keywords = [
        'recommend job', 'job recommendation', 'match job', 'job match', 
        'suitable job', 'job for me', 'job opportunity',
        'æ¨èå·¥ä½œ', 'æ¨èèŒä½', 'åŒ¹é…å·¥ä½œ', 'é€‚åˆçš„å·¥ä½œ', 'å·¥ä½œæœºä¼š',
        'find matching job', 'æ‰¾åˆ°åŒ¹é…çš„å·¥ä½œ'
    ]
    
    is_job_request = any(keyword in message_lower for keyword in job_request_keywords)
    if is_job_request:
        logger.info(f"ğŸ” Job query triggered: Job recommendation request with resume - '{message[:50]}...'")
        return True
    
    # 3. èŒä½ç›¸å…³è¯¢é—®ï¼ˆä½†æœ‰ç®€å†çš„å‰æä¸‹ï¼‰/ Job-related inquiries (with resume context)
    job_inquiry_keywords = [
        'job market', 'employment', 'career path', 'position',
        'å°±ä¸šå¸‚åœº', 'èŒä¸šå‘å±•', 'å²—ä½', 'èŒä½'
    ]
    
    is_job_inquiry = any(keyword in message_lower for keyword in job_inquiry_keywords)
    if is_job_inquiry:
        logger.info(f"ğŸ” Job query triggered: Job inquiry with resume context - '{message[:50]}...'")
        return True
    
    logger.info(f"ğŸš« Job query NOT triggered: Has resume but no job-related request - '{message[:50]}...'")
    return False

def _should_trigger_skill_heatmap(message: str, context: Dict[Any, Any]) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘æŠ€èƒ½çƒ­ç‚¹å›¾ç”Ÿæˆ / Determine if skill heatmap generation should be triggered
    æ ¹æ®READMEè¦æ±‚ï¼šä½¿ç”¨Claude 4åŸç”ŸWebSearchæœç´¢å²—ä½çƒ­ç‚¹æŠ€èƒ½å¹¶è¿›è¡Œæ·±åº¦æ€è€ƒï¼Œç„¶åè°ƒç”¨Artifactså·¥å…·ç”ŸæˆæŠ€èƒ½çƒ­ç‚¹å›¾
    """
    message_lower = message.lower()
    
    # æŠ€èƒ½çƒ­ç‚¹å›¾ç›¸å…³å…³é”®è¯ / Skill heatmap related keywords (å¤šè¯­è¨€æ”¯æŒ)
    heatmap_keywords = [
        # è‹±æ–‡
        'skills heatmap', 'skill heatmap', 'generate heatmap', 'show heatmap',
        'skill map', 'skills analysis', 'trending skills', 'hot skills',
        'skills visualization', 'skill trends', 'skills chart',
        # ä¸­æ–‡
        'æŠ€èƒ½çƒ­ç‚¹å›¾', 'æŠ€èƒ½çƒ­åŠ›å›¾', 'æŠ€èƒ½åœ°å›¾', 'ç”Ÿæˆçƒ­ç‚¹å›¾', 'æ˜¾ç¤ºçƒ­ç‚¹å›¾',
        'æŠ€èƒ½åˆ†æ', 'çƒ­é—¨æŠ€èƒ½', 'æŠ€èƒ½è¶‹åŠ¿', 'æŠ€èƒ½å›¾è¡¨', 'èƒ½åŠ›å›¾è°±',
        # å¾·è¯­ / German
        'fÃ¤higkeiten heatmap', 'skill-heatmap', 'kompetenz karte',
        'trending fÃ¤higkeiten', 'beliebte skills'
    ]
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æŠ€èƒ½çƒ­ç‚¹å›¾å…³é”®è¯ / Check for heatmap keywords
    has_heatmap_keywords = any(keyword in message_lower for keyword in heatmap_keywords)
    
    # æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸­æ˜¯å¦æœ‰æ˜ç¡®çš„çƒ­ç‚¹å›¾è¯·æ±‚ / Check context for explicit heatmap request
    force_heatmap = context.get('force_skill_heatmap', False)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯èŒä½+æŠ€èƒ½åˆ†æçš„å¤åˆè¯·æ±‚ / Check for job+skill analysis combined request
    job_skill_patterns = [
        'skills for', 'skills needed for', 'what skills', 'required skills',
        'éœ€è¦ä»€ä¹ˆæŠ€èƒ½', 'æ‰€éœ€æŠ€èƒ½', 'æŠ€èƒ½è¦æ±‚'
    ]
    has_job_skill_request = any(pattern in message_lower for pattern in job_skill_patterns)
    
    should_trigger = (
        has_heatmap_keywords or 
        force_heatmap or
        (has_job_skill_request and any(word in message_lower for word in ['developer', 'engineer', 'analyst', 'å¼€å‘', 'å·¥ç¨‹å¸ˆ', 'åˆ†æå¸ˆ']))
    )
    
    logger.info(f"ğŸ“Š Skill heatmap decision for '{message[:30]}...': "
               f"heatmap_kw={has_heatmap_keywords}, force={force_heatmap}, "
               f"job_skill={has_job_skill_request}, trigger={should_trigger}")
    
    return should_trigger

@router.post("/unified")
async def unified_chat(request: ChatRequest, http_request: Request = None):
    """
    ç»Ÿä¸€æ™ºèƒ½èŠå¤©æ¥å£ / Unified intelligent chat interface
    æ”¯æŒæ–‡ä»¶ä¸Šä¼ ã€ç®€å†åˆ†æã€èŒä½æ¨èã€æŠ€èƒ½çƒ­ç‚¹å›¾ç­‰æ‰€æœ‰åŠŸèƒ½
    
    Args:
        request: èŠå¤©è¯·æ±‚ / Chat request
        http_request: HTTPè¯·æ±‚å¯¹è±¡ / HTTP request object
        
    Returns:
        ç»Ÿä¸€èŠå¤©å“åº” / Unified chat response
    """
    try:
        claude_service = get_claude_service()
        
        # è·å–æˆ–ç”Ÿæˆä¼šè¯ID / Get or generate session ID
        session_id = None
        if request.session_id:
            session_id = request.session_id
        elif request.context and 'session_id' in request.context:
            session_id = request.context['session_id']
        else:
            session_id = str(uuid.uuid4())
            
        logger.info(f"Chat API: Using session_id = {session_id}")
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡ / Prepare context
        context = request.context or {}
        
        # å¦‚æœæœ‰æ–‡ä»¶ä¸Šä¼ ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ / If file uploaded, add to context
        if request.context and request.context.get('file_content'):
            context['file_content'] = request.context['file_content']
            context['filename'] = request.context.get('filename', 'unknown')
            context['resume_uploaded'] = True
        elif request.context and request.context.get('uploaded_file'):
            # å¤„ç†é€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ çš„æ–‡ä»¶ / Handle files uploaded via upload API
            uploaded_file = request.context['uploaded_file']
            
            # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ–°çš„document_dataæ ¼å¼ / FIX: Use new document_data format
            if uploaded_file.get('document_data'):
                context['uploaded_file'] = uploaded_file
                context['resume_uploaded'] = True
                logger.info(f"ğŸ“„ Using new document_data format for file: {uploaded_file.get('filename', 'unknown')}")
            else:
                # å‘åå…¼å®¹æ—§æ ¼å¼ / Backward compatibility with old format
                context['file_content'] = uploaded_file.get('text_content', '')
                context['filename'] = uploaded_file.get('filename', 'unknown')
                context['file_path'] = uploaded_file.get('file_path', '')
                context['resume_uploaded'] = True
                logger.info(f"ğŸ“„ Using legacy format for file: {uploaded_file.get('filename', 'unknown')}")
        
        # ğŸ”¥ ä¿®å¤ï¼šåªåœ¨ç‰¹å®šåœºæ™¯ä¸‹æŸ¥è¯¢èŒä½ / FIX: Only query jobs in specific scenarios
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘èŒä½æŸ¥è¯¢ / Check if job query should be triggered
        should_query_jobs = _should_trigger_job_query(request.message, context)
        
        # ğŸ”¥ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘æŠ€èƒ½çƒ­ç‚¹å›¾ç”Ÿæˆ / NEW: Check if skill heatmap should be triggered
        should_generate_heatmap = _should_trigger_skill_heatmap(request.message, context)
        
        # ğŸ”¥ å¦‚æœæ˜¯æŠ€èƒ½çƒ­ç‚¹å›¾è¯·æ±‚ï¼Œè®¾ç½®ä¸“é—¨çš„ä¸Šä¸‹æ–‡ / If skill heatmap request, set special context
        if should_generate_heatmap:
            context['task_type'] = 'skill_heatmap_generation'
            context['force_websearch'] = True
            logger.info(f"ğŸ“Š æŠ€èƒ½çƒ­ç‚¹å›¾è¯·æ±‚æ£€æµ‹åˆ°ï¼Œè®¾ç½®ä¸“é—¨ä¸Šä¸‹æ–‡ for session {session_id}")
        
        if should_query_jobs and http_request and hasattr(http_request, 'app'):
            db_connections = http_request.app.state.db_connections
            if 'jobs_collection' in db_connections and 'resumes_collection' in db_connections and 'openai_client' in db_connections:
                jobs_collection = db_connections['jobs_collection']
                resumes_collection = db_connections['resumes_collection']
                openai_client = db_connections['openai_client']
                
                try:
                    # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰ç®€å†å‘é‡å¯ç”¨äºæ™ºèƒ½åŒ¹é… / Check if resume vector is available for smart matching
                    user_id = context.get('session_id', 'unknown')  # ä½¿ç”¨session_idä½œä¸ºç”¨æˆ·æ ‡è¯†
                    resume_vector = None
                    resume_analysis = None
                    
                    # å°è¯•è·å–ç”¨æˆ·æœ€æ–°çš„ç®€å†å‘é‡ / Try to get user's latest resume vector
                    try:
                        user_resumes = resumes_collection.query(
                            query_embeddings=[[0.0] * 1536],  # è™šæ‹ŸæŸ¥è¯¢è·å–ç”¨æˆ·ç®€å†
                            n_results=10,
                            where={"user_id": user_id},
                            include=['embeddings', 'metadatas']  # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ˜ç¡®åŒ…å«embeddings
                        )
                        
                        if (user_resumes.get('embeddings') and 
                            len(user_resumes['embeddings']) > 0 and 
                            user_resumes['embeddings'][0] is not None and 
                            len(user_resumes['embeddings'][0]) > 0):
                            # ä½¿ç”¨æœ€æ–°ç®€å†å‘é‡ / Use latest resume vector
                            resume_vector = user_resumes['embeddings'][0][0]  # æœ€æ–°çš„ç®€å†å‘é‡
                            resume_metadata = user_resumes['metadatas'][0][0]
                            
                            # é‡å»ºç®€å†åˆ†ææ•°æ® / Rebuild resume analysis data
                            import json
                            resume_analysis = {
                                "skills": json.loads(resume_metadata.get('skills', '[]')),
                                "experience_years": resume_metadata.get('experience_years', 0),
                                "location": resume_metadata.get('location', ''),
                                "education_level": resume_metadata.get('education_level', ''),
                                "languages": json.loads(resume_metadata.get('languages', '[]')),
                                "summary": resume_metadata.get('summary', '')
                            }
                            
                            logger.info(f"ğŸ’¼ Using user resume vector for personalized job matching: {user_id}")
                    except Exception as e:
                        logger.info(f"No user resume found, using general query: {e}")
                    
                    # ğŸ”¥ æ™ºèƒ½èŒä½æŸ¥è¯¢ï¼šä¼˜å…ˆä½¿ç”¨ç®€å†å‘é‡ï¼Œå¦åˆ™ä½¿ç”¨é€šç”¨æŸ¥è¯¢ / Smart job query: prefer resume vector, fallback to general query
                    if resume_vector is not None:
                        # åŸºäºç®€å†å‘é‡çš„ä¸ªæ€§åŒ–åŒ¹é… / Personalized matching based on resume vector
                        job_results = jobs_collection.query(
                            query_embeddings=[resume_vector],
                            n_results=25  # READMEè¦æ±‚ï¼šè¿”å›25ä¸ªèŒä½
                        )
                        match_type = "personalized"
                    else:
                        # é€šç”¨èŒä½æŸ¥è¯¢ï¼ˆå›é€€æ–¹æ¡ˆï¼‰/ General job query (fallback)
                        from ..database.connection import get_text_embedding
                        query_text = "software engineer developer germany"
                        query_embedding = get_text_embedding(openai_client, query_text)
                        
                        job_results = jobs_collection.query(
                            query_embeddings=[query_embedding],
                            n_results=25
                        )
                        match_type = "general"
                    
                    # æ„å»ºèŒä½åˆ—è¡¨ / Build job list
                    if job_results['metadatas']:
                        jobs = []
                        for metadata in job_results['metadatas'][0]:
                            try:
                                job_data = {
                                    "id": metadata.get('id', ''),
                                    "title": metadata.get('title', ''),
                                    "company_name": metadata.get('company_name', ''),
                                    "location": metadata.get('location', ''),
                                    "description": metadata.get('description', ''),
                                    "url": metadata.get('url', ''),
                                    "job_type": metadata.get('job_type', 'æœªçŸ¥')
                                }
                                jobs.append(job_data)
                            except:
                                continue
                        
                        if jobs:
                            context['job_postings'] = jobs
                            context['match_type'] = match_type
                            context['resume_analysis'] = resume_analysis  # ä¼ é€’ç®€å†åˆ†ææ•°æ®
                            logger.info(f"ğŸ’¼ Enhanced message with job postings context for session {session_id}: {len(jobs)} jobs ({match_type} matching)")
                            
                except Exception as e:
                    logger.warning(f"Failed to fetch job data: {e}")
        
        async def generate_response():
            """ç”Ÿæˆç»Ÿä¸€æµå¼å“åº” / Generate unified streaming response"""
            import json  # ğŸ”¥ ä¿®å¤ä½œç”¨åŸŸé—®é¢˜ï¼šåœ¨åµŒå¥—å‡½æ•°ä¸­é‡æ–°å¯¼å…¥json
            try:
                # ä½¿ç”¨ç»Ÿä¸€çš„ClaudeæœåŠ¡ / Use unified Claude service
                async for event in claude_service.chat_stream_unified(
                    request.message, 
                    context, 
                    session_id
                ):
                    # è½¬æ¢äº‹ä»¶æ ¼å¼ / Convert event format
                    response_event = {
                        "session_id": session_id,
                        "timestamp": event.get("timestamp", ""),
                        **event
                    }
                    
                    yield f"data: {json.dumps(response_event)}\n\n"
                    
                # å‘é€ç»“æŸä¿¡å· / Send end signal
                yield f"data: {json.dumps({'type': 'complete', 'session_id': session_id})}\n\n"
                
            except Exception as e:
                logger.error(f"Unified streaming error: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': f'Error: {str(e)}', 'session_id': session_id})}\n\n"
        
        return StreamingResponse(
            generate_response(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*"
            }
        )
        
    except Exception as e:
        logger.error(f"Unified chat failed: {e}")
        raise HTTPException(status_code=500, detail=f"ç»Ÿä¸€èŠå¤©å¤±è´¥: {str(e)}")


@router.post("/message")
async def chat_message(request: ChatRequest, http_request: Request = None):
    """
    éæµå¼èŠå¤©æ¶ˆæ¯ / Non-streaming chat message
    å…¼å®¹æ—§æ¥å£ / Backward compatibility
    
    Args:
        request: èŠå¤©è¯·æ±‚ / Chat request
        http_request: HTTPè¯·æ±‚å¯¹è±¡ / HTTP request object
        
    Returns:
        èŠå¤©å“åº” / Chat response
    """
    try:
        claude_service = get_claude_service()
        
        # è·å–æˆ–ç”Ÿæˆä¼šè¯ID / Get or generate session ID
        session_id = None
        if request.session_id:
            session_id = request.session_id
        elif request.context and 'session_id' in request.context:
            session_id = request.context['session_id']
        else:
            session_id = str(uuid.uuid4())
            
        logger.info(f"Chat API: Using session_id = {session_id}")
        
        # ğŸ”¥ å‡†å¤‡ä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬ç®€å†å‘é‡æŸ¥è¯¢ / Prepare context including resume vector query
        context = request.context or {}
        
        # ğŸ”¥ ä¿®å¤ï¼šåªåœ¨ç‰¹å®šåœºæ™¯ä¸‹æŸ¥è¯¢èŒä½ / FIX: Only query jobs in specific scenarios
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘èŒä½æŸ¥è¯¢ / Check if job query should be triggered
        should_query_jobs = _should_trigger_job_query(request.message, context)
        
        if should_query_jobs and http_request and hasattr(http_request, 'app'):
            db_connections = http_request.app.state.db_connections
            if 'jobs_collection' in db_connections and 'resumes_collection' in db_connections and 'openai_client' in db_connections:
                jobs_collection = db_connections['jobs_collection']
                resumes_collection = db_connections['resumes_collection']
                openai_client = db_connections['openai_client']
                
                try:
                    # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰ç®€å†å‘é‡å¯ç”¨äºæ™ºèƒ½åŒ¹é… / Check if resume vector is available for smart matching
                    user_id = context.get('session_id', session_id)  # ä½¿ç”¨session_idä½œä¸ºç”¨æˆ·æ ‡è¯†
                    resume_vector = None
                    resume_analysis = None
                    
                    # å°è¯•è·å–ç”¨æˆ·æœ€æ–°çš„ç®€å†å‘é‡ / Try to get user's latest resume vector
                    try:
                        user_resumes = resumes_collection.query(
                            query_embeddings=[[0.0] * 1536],  # è™šæ‹ŸæŸ¥è¯¢è·å–ç”¨æˆ·ç®€å†
                            n_results=10,
                            where={"user_id": user_id},
                            include=['embeddings', 'metadatas']  # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ˜ç¡®åŒ…å«embeddings
                        )
                        
                        if (user_resumes.get('embeddings') and 
                            len(user_resumes['embeddings']) > 0 and 
                            user_resumes['embeddings'][0] is not None and 
                            len(user_resumes['embeddings'][0]) > 0):
                            # ä½¿ç”¨æœ€æ–°ç®€å†å‘é‡ / Use latest resume vector
                            resume_vector = user_resumes['embeddings'][0][0]  # æœ€æ–°çš„ç®€å†å‘é‡
                            resume_metadata = user_resumes['metadatas'][0][0]
                            
                            # é‡å»ºç®€å†åˆ†ææ•°æ® / Rebuild resume analysis data
                            import json
                            resume_analysis = {
                                "skills": json.loads(resume_metadata.get('skills', '[]')),
                                "experience_years": resume_metadata.get('experience_years', 0),
                                "location": resume_metadata.get('location', ''),
                                "education_level": resume_metadata.get('education_level', ''),
                                "languages": json.loads(resume_metadata.get('languages', '[]')),
                                "summary": resume_metadata.get('summary', '')
                            }
                            
                            logger.info(f"ğŸ’¼ Using user resume vector for personalized job matching: {user_id}")
                    except Exception as e:
                        logger.info(f"No user resume found, using general query: {e}")
                    
                    # ğŸ”¥ æ™ºèƒ½èŒä½æŸ¥è¯¢ï¼šä¼˜å…ˆä½¿ç”¨ç®€å†å‘é‡ï¼Œå¦åˆ™ä½¿ç”¨é€šç”¨æŸ¥è¯¢ / Smart job query: prefer resume vector, fallback to general query
                    if resume_vector is not None:
                        # åŸºäºç®€å†å‘é‡çš„ä¸ªæ€§åŒ–åŒ¹é… / Personalized matching based on resume vector
                        job_results = jobs_collection.query(
                            query_embeddings=[resume_vector],
                            n_results=25  # READMEè¦æ±‚ï¼šè¿”å›25ä¸ªèŒä½
                        )
                        match_type = "personalized"
                    else:
                        # é€šç”¨èŒä½æŸ¥è¯¢ï¼ˆå›é€€æ–¹æ¡ˆï¼‰/ General job query (fallback)
                        from ..database.connection import get_text_embedding
                        query_text = "software engineer developer germany"
                        query_embedding = get_text_embedding(openai_client, query_text)
                        
                        job_results = jobs_collection.query(
                            query_embeddings=[query_embedding],
                            n_results=25
                        )
                        match_type = "general"
                    
                    # æ„å»ºèŒä½åˆ—è¡¨ / Build job list
                    if job_results['metadatas']:
                        jobs = []
                        for metadata in job_results['metadatas'][0]:
                            try:
                                job_data = {
                                    "id": metadata.get('id', ''),
                                    "title": metadata.get('title', ''),
                                    "company_name": metadata.get('company_name', ''),
                                    "location": metadata.get('location', ''),
                                    "description": metadata.get('description', ''),
                                    "url": metadata.get('url', ''),
                                    "job_type": metadata.get('job_type', 'æœªçŸ¥')
                                }
                                jobs.append(job_data)
                            except:
                                continue
                        
                        if jobs:
                            context['job_postings'] = jobs
                            context['match_type'] = match_type
                            context['resume_analysis'] = resume_analysis  # ä¼ é€’ç®€å†åˆ†ææ•°æ®
                            logger.info(f"ğŸ’¼ Enhanced message with job postings context for session {session_id}: {len(jobs)} jobs ({match_type} matching)")
                            
                except Exception as e:
                    logger.warning(f"Failed to fetch job data: {e}")
        
        # æ”¶é›†å®Œæ•´å“åº” / Collect complete response
        complete_response = ""
        tool_results = []
        
        async for event in claude_service.chat_stream_unified(
            request.message, 
            context, 
            session_id
        ):
            if event.get("type") == "text_delta":
                complete_response += event.get("content", "")
            elif event.get("type") == "tool_execution_complete":
                tool_results.append({
                    "tool_name": event.get("tool_name"),
                    "result": event.get("result")
                })
        
        return ChatResponse(
            response=complete_response,
            message_type="text",
            data={
                "session_id": session_id,
                "tool_results": tool_results,
                **(context or {})
            }
        )
        
    except Exception as e:
        logger.error(f"Chat message failed: {e}")
        raise HTTPException(status_code=500, detail=f"èŠå¤©å¤±è´¥: {str(e)}")


@router.post("/stream")
async def chat_stream(request: ChatRequest):
    """
    æµå¼èŠå¤© / Streaming chat
    å…¼å®¹æ—§æ¥å£ï¼Œé‡å®šå‘åˆ°ç»Ÿä¸€æ¥å£ / Backward compatibility, redirect to unified interface
    
    Args:
        request: èŠå¤©è¯·æ±‚ / Chat request
        
    Returns:
        æµå¼èŠå¤©å“åº” / Streaming chat response
    """
    try:
        # é‡å®šå‘åˆ°ç»Ÿä¸€æ¥å£ / Redirect to unified interface
        return await unified_chat(request)
        
    except Exception as e:
        logger.error(f"Streaming chat failed: {e}")
        raise HTTPException(status_code=500, detail=f"æµå¼èŠå¤©å¤±è´¥: {str(e)}")


# å‘åå…¼å®¹çš„ç‹¬ç«‹ç«¯ç‚¹ / Backward compatibility standalone endpoints
@router.post("/analyze-resume")
async def analyze_resume_standalone(
    user_id: str,
    resume_content: str,
    filename: str,
    request: Request
):
    """
    ç‹¬ç«‹ç®€å†åˆ†æç«¯ç‚¹ / Standalone resume analysis endpoint
    å‘åå…¼å®¹ï¼Œå»ºè®®ä½¿ç”¨ç»Ÿä¸€èŠå¤©æ¥å£ / Backward compatibility, recommend using unified chat interface
    """
    try:
        logger.warning("Using deprecated standalone resume analysis endpoint. Please use unified chat interface.")
        
        claude_service = get_claude_service()
        
        # æ„å»ºèŠå¤©è¯·æ±‚ / Build chat request
        message = f"è¯·åˆ†ææˆ‘çš„ç®€å†æ–‡ä»¶ï¼š{filename}ã€‚è¯·æä¾›è¯¦ç»†çš„åˆ†æå’Œæ”¹è¿›å»ºè®®ã€‚"
        context = {
            "file_content": resume_content,
            "filename": filename,
            "resume_uploaded": True,
            "user_id": user_id
        }
        
        # ä½¿ç”¨ç»Ÿä¸€æœåŠ¡ / Use unified service
        session_id = str(uuid.uuid4())
        analysis_result = None
        job_matches = []
        
        async for event in claude_service.chat_stream_unified(message, context, session_id):
            if event.get("type") == "tool_execution_complete":
                tool_name = event.get("tool_name")
                result = event.get("result")
                
                if tool_name == "analyze_resume":
                    analysis_result = result
                elif tool_name == "match_jobs_with_resume":
                    job_matches = result.get('matches', [])
        
        if not analysis_result:
            raise HTTPException(status_code=400, detail="ç®€å†åˆ†æå¤±è´¥")
        
        return {
            "analysis": analysis_result,
            "job_matches": job_matches,
            "message_type": "resume_analysis",
            "total_matches": len(job_matches),
            "session_id": session_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Standalone resume analysis failed: {e}")
        raise HTTPException(status_code=500, detail=f"ç®€å†åˆ†æå¤±è´¥: {str(e)}")


@router.get("/skill-heatmap/{job_title}")
async def generate_skill_heatmap_standalone(job_title: str):
    """
    ç‹¬ç«‹æŠ€èƒ½çƒ­ç‚¹å›¾ç«¯ç‚¹ / Standalone skill heatmap endpoint
    å‘åå…¼å®¹ï¼Œå»ºè®®ä½¿ç”¨ç»Ÿä¸€èŠå¤©æ¥å£ / Backward compatibility, recommend using unified chat interface
    ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•è°ƒç”¨ / FIXED: Use correct method call
    """
    try:
        logger.warning("Using deprecated standalone skill heatmap endpoint. Please use unified chat interface.")
        
        claude_service = get_claude_service()
        
        # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å / FIXED: Use correct method name
        heatmap_data = await claude_service.generate_skill_heatmap_data(job_title)
        
        return {
            "success": heatmap_data.get("success", True),
            "heatmap_data": heatmap_data,
            "message_type": "skill_heatmap",
            "job_title": job_title,
            "artifacts_generated": heatmap_data.get("artifacts_generated", False),
            "interactive": heatmap_data.get("interactive", False),
            "websearch_used": heatmap_data.get("websearch_used", False)
        }
        
    except Exception as e:
        logger.error(f"Standalone skill heatmap generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"æŠ€èƒ½çƒ­ç‚¹å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")


@router.get("/market-insights")
async def get_market_insights_standalone(query: str):
    """
    ç‹¬ç«‹å¸‚åœºæ´å¯Ÿç«¯ç‚¹ / Standalone market insights endpoint
    å‘åå…¼å®¹ï¼Œå»ºè®®ä½¿ç”¨ç»Ÿä¸€èŠå¤©æ¥å£ / Backward compatibility, recommend using unified chat interface
    ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•è°ƒç”¨ / FIXED: Use correct method call
    """
    try:
        logger.warning("Using deprecated standalone market insights endpoint. Please use unified chat interface.")
        
        claude_service = get_claude_service()
        
        # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å / FIXED: Use correct method name
        insights_data = await claude_service.get_german_job_market_insights(query)
        
        return {
            "insights": insights_data,
            "message_type": "market_insights",
            "query": query,
            "success": True
        }
        
    except Exception as e:
        logger.error(f"Standalone market insights failed: {e}")
        raise HTTPException(status_code=500, detail=f"å¸‚åœºæ´å¯Ÿå¤±è´¥: {str(e)}")


@router.get("/history/{user_id}")
async def get_chat_history(user_id: str, limit: int = 20):
    """
    è·å–èŠå¤©å†å² / Get chat history
    
    Args:
        user_id: ç”¨æˆ·ID / User ID
        limit: é™åˆ¶æ•°é‡ / Limit count
        
    Returns:
        èŠå¤©å†å² / Chat history
    """
    try:
        # TODO: ä»æ•°æ®åº“è·å–èŠå¤©å†å² / Get chat history from database
        # ç›®å‰è¿”å›ç©ºå†å²ï¼Œæœªæ¥å¯ä»¥ä¸SQLiteé›†æˆ / Currently return empty history, can integrate with SQLite in future
        
        return {
            "messages": [],
            "total_count": 0,
            "user_id": user_id,
            "sessions": []
        }
        
    except Exception as e:
        logger.error(f"Failed to get chat history: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–èŠå¤©å†å²å¤±è´¥: {str(e)}")


@router.delete("/session/{session_id}")
async def clear_session(session_id: str):
    """
    æ¸…é™¤ä¼šè¯çŠ¶æ€ / Clear session state
    
    Args:
        session_id: ä¼šè¯ID / Session ID
        
    Returns:
        æ¸…é™¤ç»“æœ / Clear result
    """
    try:
        claude_service = get_claude_service()
        
        # æ¸…é™¤ä¼šè¯çŠ¶æ€ / Clear session state
        if session_id in claude_service.session_state:
            del claude_service.session_state[session_id]
            
        return {
            "success": True,
            "message": f"Session {session_id} cleared",
            "session_id": session_id
        }
        
    except Exception as e:
        logger.error(f"Failed to clear session: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤ä¼šè¯å¤±è´¥: {str(e)}")


@router.get("/sessions")
async def list_sessions():
    """
    åˆ—å‡ºæ‰€æœ‰æ´»è·ƒä¼šè¯ / List all active sessions
    
    Returns:
        ä¼šè¯åˆ—è¡¨ / Session list
    """
    try:
        claude_service = get_claude_service()
        
        sessions = []
        for session_id, session_data in claude_service.session_state.items():
            sessions.append({
                "session_id": session_id,
                "messages_count": len(session_data.get("messages", [])),
                "has_resume_analysis": session_data.get("resume_analysis") is not None,
                "job_postings_count": len(session_data.get("job_postings", []))
            })
        
        return {
            "sessions": sessions,
            "total_sessions": len(sessions)
        }
        
    except Exception as e:
        logger.error(f"Failed to list sessions: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {str(e)}") 


@router.get("/token-usage")
async def get_token_usage(session_id: Optional[str] = None):
    """
    è·å–tokenä½¿ç”¨ç»Ÿè®¡ / Get token usage statistics
    
    Args:
        session_id: å¯é€‰çš„ä¼šè¯ID / Optional session ID
        
    Returns:
        Tokenä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯ / Token usage statistics
    """
    try:
        claude_service = get_claude_service()
        usage_stats = await claude_service.get_token_usage_stats(session_id)
        
        return {
            "status": "success",
            "data": usage_stats,
            "message": "Token usage statistics retrieved successfully"
        }
        
    except Exception as e:
        logger.error(f"Failed to get token usage: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve token usage statistics: {str(e)}"
        )


@router.post("/budget-alert")
async def check_budget_alert(daily_budget: float = 5.0):
    """
    æ£€æŸ¥é¢„ç®—è­¦å‘Š / Check budget alerts
    
    Args:
        daily_budget: æ¯æ—¥é¢„ç®—é™åˆ¶ (USD) / Daily budget limit in USD
        
    Returns:
        é¢„ç®—çŠ¶æ€ä¿¡æ¯ / Budget status information
    """
    try:
        claude_service = get_claude_service()
        budget_status = claude_service.token_tracker.check_budget_alert(daily_budget)
        
        return {
            "status": "success",
            "data": budget_status,
            "message": f"Budget status: {budget_status['alert_level']}"
        }
        
    except Exception as e:
        logger.error(f"Failed to check budget: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to check budget status: {str(e)}"
        ) 