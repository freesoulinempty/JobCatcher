"""
AIèŠå¤©API / AI chat API
ç»Ÿä¸€çš„æ™ºèƒ½èŠå¤©æ¥å£ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨ / Unified intelligent chat interface with tool calling
"""

from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
from typing import Optional, Dict, Any
import logging
import json
import uuid
import time

from ..models.user import ChatRequest, ChatResponse
from ..services.claude_service import ClaudeService
from ..core.logging import log_api_access, log_performance, log_error_with_context, get_logger

logger = get_logger("JobCatcher.api.chat")
router = APIRouter()

# å…¨å±€ClaudeServiceå®ä¾‹ï¼Œä¿æŒä¼šè¯çŠ¶æ€ / Global ClaudeService instance to maintain session state
_global_claude_service = None

def get_claude_service() -> ClaudeService:
    """è·å–å…¨å±€ClaudeServiceå®ä¾‹ / Get global ClaudeService instance"""
    global _global_claude_service
    if _global_claude_service is None:
        _global_claude_service = ClaudeService()
        logger.info("Created global ClaudeService instance for session management")
    return _global_claude_service

@router.post("/unified")
async def unified_chat(request: ChatRequest, http_request: Request = None):
    """
    ç»Ÿä¸€æ™ºèƒ½èŠå¤©æ¥å£ / Unified intelligent chat interface
    æ”¯æŒæ–‡ä»¶ä¸Šä¼ ã€ç®€å†åˆ†æã€èŒä½æ¨èã€æŠ€èƒ½çƒ­ç‚¹å›¾ç­‰æ‰€æœ‰åŠŸèƒ½
    
    Args:
        request: èŠå¤©è¯·æ±‚ / Chat request
        http_request: HTTPè¯·æ±‚å¯¹è±¡ / HTTP request object
        
    Returns:
        ç»Ÿä¸€èŠå¤©å“åº” / Unified chat response
    """
    try:
        claude_service = get_claude_service()
        
        # è·å–æˆ–ç”Ÿæˆä¼šè¯ID / Get or generate session ID
        session_id = None
        if request.session_id:
            session_id = request.session_id
        elif request.context and 'session_id' in request.context:
            session_id = request.context['session_id']
        else:
            session_id = str(uuid.uuid4())
            
        logger.info(f"Chat API: Using session_id = {session_id}")
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡ / Prepare context
        context = request.context or {}
        
        # å¦‚æœæœ‰æ–‡ä»¶ä¸Šä¼ ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ / If file uploaded, add to context
        if request.context and request.context.get('file_content'):
            context['file_content'] = request.context['file_content']
            context['filename'] = request.context.get('filename', 'unknown')
            context['resume_uploaded'] = True
        elif request.context and request.context.get('uploaded_file'):
            # å¤„ç†é€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ çš„æ–‡ä»¶ / Handle files uploaded via upload API
            uploaded_file = request.context['uploaded_file']
            
            # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ–°çš„document_dataæ ¼å¼ / FIX: Use new document_data format
            if uploaded_file.get('document_data'):
                context['uploaded_file'] = uploaded_file
                context['resume_uploaded'] = True
                logger.info(f"ğŸ“„ Using new document_data format for file: {uploaded_file.get('filename', 'unknown')}")
            else:
                # å‘åå…¼å®¹æ—§æ ¼å¼ / Backward compatibility with old format
                context['file_content'] = uploaded_file.get('text_content', '')
                context['filename'] = uploaded_file.get('filename', 'unknown')
                context['file_path'] = uploaded_file.get('file_path', '')
                context['resume_uploaded'] = True
                logger.info(f"ğŸ“„ Using legacy format for file: {uploaded_file.get('filename', 'unknown')}")
        
        # ğŸ”¥ READMEæ­¥éª¤4ï¼šåŸºäºç®€å†å‘é‡æŸ¥è¯¢åŒ¹é…èŒä½ / README step 4: Query matching jobs based on resume vector
        if http_request and hasattr(http_request, 'app'):
            db_connections = http_request.app.state.db_connections
            if 'jobs_collection' in db_connections and 'resumes_collection' in db_connections and 'openai_client' in db_connections:
                jobs_collection = db_connections['jobs_collection']
                resumes_collection = db_connections['resumes_collection']
                openai_client = db_connections['openai_client']
                
                try:
                    # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰ç®€å†å‘é‡å¯ç”¨äºæ™ºèƒ½åŒ¹é… / Check if resume vector is available for smart matching
                    user_id = context.get('session_id', 'unknown')  # ä½¿ç”¨session_idä½œä¸ºç”¨æˆ·æ ‡è¯†
                    resume_vector = None
                    resume_analysis = None
                    
                    # å°è¯•è·å–ç”¨æˆ·æœ€æ–°çš„ç®€å†å‘é‡ / Try to get user's latest resume vector
                    try:
                        user_resumes = resumes_collection.query(
                            query_embeddings=[[0.0] * 1536],  # è™šæ‹ŸæŸ¥è¯¢è·å–ç”¨æˆ·ç®€å†
                            n_results=10,
                            where={"user_id": user_id},
                            include=['embeddings', 'metadatas']  # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ˜ç¡®åŒ…å«embeddings
                        )
                        
                        if (user_resumes.get('embeddings') and 
                            len(user_resumes['embeddings']) > 0 and 
                            user_resumes['embeddings'][0] is not None and 
                            len(user_resumes['embeddings'][0]) > 0):
                            # ä½¿ç”¨æœ€æ–°ç®€å†å‘é‡ / Use latest resume vector
                            resume_vector = user_resumes['embeddings'][0][0]  # æœ€æ–°çš„ç®€å†å‘é‡
                            resume_metadata = user_resumes['metadatas'][0][0]
                            
                            # é‡å»ºç®€å†åˆ†ææ•°æ® / Rebuild resume analysis data
                            import json
                            resume_analysis = {
                                "skills": json.loads(resume_metadata.get('skills', '[]')),
                                "experience_years": resume_metadata.get('experience_years', 0),
                                "location": resume_metadata.get('location', ''),
                                "education_level": resume_metadata.get('education_level', ''),
                                "languages": json.loads(resume_metadata.get('languages', '[]')),
                                "summary": resume_metadata.get('summary', '')
                            }
                            
                            logger.info(f"ğŸ’¼ Using user resume vector for personalized job matching: {user_id}")
                    except Exception as e:
                        logger.info(f"No user resume found, using general query: {e}")
                    
                    # ğŸ”¥ æ™ºèƒ½èŒä½æŸ¥è¯¢ï¼šä¼˜å…ˆä½¿ç”¨ç®€å†å‘é‡ï¼Œå¦åˆ™ä½¿ç”¨é€šç”¨æŸ¥è¯¢ / Smart job query: prefer resume vector, fallback to general query
                    if resume_vector is not None:
                        # åŸºäºç®€å†å‘é‡çš„ä¸ªæ€§åŒ–åŒ¹é… / Personalized matching based on resume vector
                        job_results = jobs_collection.query(
                            query_embeddings=[resume_vector],
                            n_results=25  # READMEè¦æ±‚ï¼šè¿”å›25ä¸ªèŒä½
                        )
                        match_type = "personalized"
                    else:
                        # é€šç”¨èŒä½æŸ¥è¯¢ï¼ˆå›é€€æ–¹æ¡ˆï¼‰/ General job query (fallback)
                        from ..database.connection import get_text_embedding
                        query_text = "software engineer developer germany"
                        query_embedding = get_text_embedding(openai_client, query_text)
                        
                        job_results = jobs_collection.query(
                            query_embeddings=[query_embedding],
                            n_results=25
                        )
                        match_type = "general"
                    
                    # æ„å»ºèŒä½åˆ—è¡¨ / Build job list
                    if job_results['metadatas']:
                        jobs = []
                        for metadata in job_results['metadatas'][0]:
                            try:
                                job_data = {
                                    "id": metadata.get('id', ''),
                                    "title": metadata.get('title', ''),
                                    "company_name": metadata.get('company_name', ''),
                                    "location": metadata.get('location', ''),
                                    "description": metadata.get('description', ''),
                                    "url": metadata.get('url', ''),
                                    "job_type": metadata.get('job_type', 'æœªçŸ¥')
                                }
                                jobs.append(job_data)
                            except:
                                continue
                        
                        if jobs:
                            context['job_postings'] = jobs
                            context['match_type'] = match_type
                            context['resume_analysis'] = resume_analysis  # ä¼ é€’ç®€å†åˆ†ææ•°æ®
                            logger.info(f"ğŸ’¼ Enhanced message with job postings context for session {session_id}: {len(jobs)} jobs ({match_type} matching)")
                            
                except Exception as e:
                    logger.warning(f"Failed to fetch job data: {e}")
        
        async def generate_response():
            """ç”Ÿæˆç»Ÿä¸€æµå¼å“åº” / Generate unified streaming response"""
            import json  # ğŸ”¥ ä¿®å¤ä½œç”¨åŸŸé—®é¢˜ï¼šåœ¨åµŒå¥—å‡½æ•°ä¸­é‡æ–°å¯¼å…¥json
            try:
                # ä½¿ç”¨ç»Ÿä¸€çš„ClaudeæœåŠ¡ / Use unified Claude service
                async for event in claude_service.chat_stream_unified(
                    request.message, 
                    context, 
                    session_id
                ):
                    # è½¬æ¢äº‹ä»¶æ ¼å¼ / Convert event format
                    response_event = {
                        "session_id": session_id,
                        "timestamp": event.get("timestamp", ""),
                        **event
                    }
                    
                    yield f"data: {json.dumps(response_event)}\n\n"
                    
                # å‘é€ç»“æŸä¿¡å· / Send end signal
                yield f"data: {json.dumps({'type': 'complete', 'session_id': session_id})}\n\n"
                
            except Exception as e:
                logger.error(f"Unified streaming error: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': f'Error: {str(e)}', 'session_id': session_id})}\n\n"
        
        return StreamingResponse(
            generate_response(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*"
            }
        )
        
    except Exception as e:
        logger.error(f"Unified chat failed: {e}")
        raise HTTPException(status_code=500, detail=f"ç»Ÿä¸€èŠå¤©å¤±è´¥: {str(e)}")


@router.post("/message")
async def chat_message(request: ChatRequest, http_request: Request = None):
    """
    éæµå¼èŠå¤©æ¶ˆæ¯ / Non-streaming chat message
    å…¼å®¹æ—§æ¥å£ / Backward compatibility
    
    Args:
        request: èŠå¤©è¯·æ±‚ / Chat request
        http_request: HTTPè¯·æ±‚å¯¹è±¡ / HTTP request object
        
    Returns:
        èŠå¤©å“åº” / Chat response
    """
    try:
        claude_service = get_claude_service()
        
        # è·å–æˆ–ç”Ÿæˆä¼šè¯ID / Get or generate session ID
        session_id = None
        if request.session_id:
            session_id = request.session_id
        elif request.context and 'session_id' in request.context:
            session_id = request.context['session_id']
        else:
            session_id = str(uuid.uuid4())
            
        logger.info(f"Chat API: Using session_id = {session_id}")
        
        # ğŸ”¥ å‡†å¤‡ä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬ç®€å†å‘é‡æŸ¥è¯¢ / Prepare context including resume vector query
        context = request.context or {}
        
        # ğŸ”¥ READMEæ­¥éª¤4ï¼šåŸºäºç®€å†å‘é‡æŸ¥è¯¢åŒ¹é…èŒä½ / README step 4: Query matching jobs based on resume vector
        if http_request and hasattr(http_request, 'app'):
            db_connections = http_request.app.state.db_connections
            if 'jobs_collection' in db_connections and 'resumes_collection' in db_connections and 'openai_client' in db_connections:
                jobs_collection = db_connections['jobs_collection']
                resumes_collection = db_connections['resumes_collection']
                openai_client = db_connections['openai_client']
                
                try:
                    # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰ç®€å†å‘é‡å¯ç”¨äºæ™ºèƒ½åŒ¹é… / Check if resume vector is available for smart matching
                    user_id = context.get('session_id', session_id)  # ä½¿ç”¨session_idä½œä¸ºç”¨æˆ·æ ‡è¯†
                    resume_vector = None
                    resume_analysis = None
                    
                    # å°è¯•è·å–ç”¨æˆ·æœ€æ–°çš„ç®€å†å‘é‡ / Try to get user's latest resume vector
                    try:
                        user_resumes = resumes_collection.query(
                            query_embeddings=[[0.0] * 1536],  # è™šæ‹ŸæŸ¥è¯¢è·å–ç”¨æˆ·ç®€å†
                            n_results=10,
                            where={"user_id": user_id},
                            include=['embeddings', 'metadatas']  # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ˜ç¡®åŒ…å«embeddings
                        )
                        
                        if (user_resumes.get('embeddings') and 
                            len(user_resumes['embeddings']) > 0 and 
                            user_resumes['embeddings'][0] is not None and 
                            len(user_resumes['embeddings'][0]) > 0):
                            # ä½¿ç”¨æœ€æ–°ç®€å†å‘é‡ / Use latest resume vector
                            resume_vector = user_resumes['embeddings'][0][0]  # æœ€æ–°çš„ç®€å†å‘é‡
                            resume_metadata = user_resumes['metadatas'][0][0]
                            
                            # é‡å»ºç®€å†åˆ†ææ•°æ® / Rebuild resume analysis data
                            import json
                            resume_analysis = {
                                "skills": json.loads(resume_metadata.get('skills', '[]')),
                                "experience_years": resume_metadata.get('experience_years', 0),
                                "location": resume_metadata.get('location', ''),
                                "education_level": resume_metadata.get('education_level', ''),
                                "languages": json.loads(resume_metadata.get('languages', '[]')),
                                "summary": resume_metadata.get('summary', '')
                            }
                            
                            logger.info(f"ğŸ’¼ Using user resume vector for personalized job matching: {user_id}")
                    except Exception as e:
                        logger.info(f"No user resume found, using general query: {e}")
                    
                    # ğŸ”¥ æ™ºèƒ½èŒä½æŸ¥è¯¢ï¼šä¼˜å…ˆä½¿ç”¨ç®€å†å‘é‡ï¼Œå¦åˆ™ä½¿ç”¨é€šç”¨æŸ¥è¯¢ / Smart job query: prefer resume vector, fallback to general query
                    if resume_vector is not None:
                        # åŸºäºç®€å†å‘é‡çš„ä¸ªæ€§åŒ–åŒ¹é… / Personalized matching based on resume vector
                        job_results = jobs_collection.query(
                            query_embeddings=[resume_vector],
                            n_results=25  # READMEè¦æ±‚ï¼šè¿”å›25ä¸ªèŒä½
                        )
                        match_type = "personalized"
                    else:
                        # é€šç”¨èŒä½æŸ¥è¯¢ï¼ˆå›é€€æ–¹æ¡ˆï¼‰/ General job query (fallback)
                        from ..database.connection import get_text_embedding
                        query_text = "software engineer developer germany"
                        query_embedding = get_text_embedding(openai_client, query_text)
                        
                        job_results = jobs_collection.query(
                            query_embeddings=[query_embedding],
                            n_results=25
                        )
                        match_type = "general"
                    
                    # æ„å»ºèŒä½åˆ—è¡¨ / Build job list
                    if job_results['metadatas']:
                        jobs = []
                        for metadata in job_results['metadatas'][0]:
                            try:
                                job_data = {
                                    "id": metadata.get('id', ''),
                                    "title": metadata.get('title', ''),
                                    "company_name": metadata.get('company_name', ''),
                                    "location": metadata.get('location', ''),
                                    "description": metadata.get('description', ''),
                                    "url": metadata.get('url', ''),
                                    "job_type": metadata.get('job_type', 'æœªçŸ¥')
                                }
                                jobs.append(job_data)
                            except:
                                continue
                        
                        if jobs:
                            context['job_postings'] = jobs
                            context['match_type'] = match_type
                            context['resume_analysis'] = resume_analysis  # ä¼ é€’ç®€å†åˆ†ææ•°æ®
                            logger.info(f"ğŸ’¼ Enhanced message with job postings context for session {session_id}: {len(jobs)} jobs ({match_type} matching)")
                            
                except Exception as e:
                    logger.warning(f"Failed to fetch job data: {e}")
        
        # æ”¶é›†å®Œæ•´å“åº” / Collect complete response
        complete_response = ""
        tool_results = []
        
        async for event in claude_service.chat_stream_unified(
            request.message, 
            context, 
            session_id
        ):
            if event.get("type") == "text_delta":
                complete_response += event.get("content", "")
            elif event.get("type") == "tool_execution_complete":
                tool_results.append({
                    "tool_name": event.get("tool_name"),
                    "result": event.get("result")
                })
        
        return ChatResponse(
            response=complete_response,
            message_type="text",
            data={
                "session_id": session_id,
                "tool_results": tool_results,
                **(context or {})
            }
        )
        
    except Exception as e:
        logger.error(f"Chat message failed: {e}")
        raise HTTPException(status_code=500, detail=f"èŠå¤©å¤±è´¥: {str(e)}")


@router.post("/stream")
async def chat_stream(request: ChatRequest):
    """
    æµå¼èŠå¤© / Streaming chat
    å…¼å®¹æ—§æ¥å£ï¼Œé‡å®šå‘åˆ°ç»Ÿä¸€æ¥å£ / Backward compatibility, redirect to unified interface
    
    Args:
        request: èŠå¤©è¯·æ±‚ / Chat request
        
    Returns:
        æµå¼èŠå¤©å“åº” / Streaming chat response
    """
    try:
        # é‡å®šå‘åˆ°ç»Ÿä¸€æ¥å£ / Redirect to unified interface
        return await unified_chat(request)
        
    except Exception as e:
        logger.error(f"Streaming chat failed: {e}")
        raise HTTPException(status_code=500, detail=f"æµå¼èŠå¤©å¤±è´¥: {str(e)}")


# å‘åå…¼å®¹çš„ç‹¬ç«‹ç«¯ç‚¹ / Backward compatibility standalone endpoints
@router.post("/analyze-resume")
async def analyze_resume_standalone(
    user_id: str,
    resume_content: str,
    filename: str,
    request: Request
):
    """
    ç‹¬ç«‹ç®€å†åˆ†æç«¯ç‚¹ / Standalone resume analysis endpoint
    å‘åå…¼å®¹ï¼Œå»ºè®®ä½¿ç”¨ç»Ÿä¸€èŠå¤©æ¥å£ / Backward compatibility, recommend using unified chat interface
    """
    try:
        logger.warning("Using deprecated standalone resume analysis endpoint. Please use unified chat interface.")
        
        claude_service = get_claude_service()
        
        # æ„å»ºèŠå¤©è¯·æ±‚ / Build chat request
        message = f"è¯·åˆ†ææˆ‘çš„ç®€å†æ–‡ä»¶ï¼š{filename}ã€‚è¯·æä¾›è¯¦ç»†çš„åˆ†æå’Œæ”¹è¿›å»ºè®®ã€‚"
        context = {
            "file_content": resume_content,
            "filename": filename,
            "resume_uploaded": True,
            "user_id": user_id
        }
        
        # ä½¿ç”¨ç»Ÿä¸€æœåŠ¡ / Use unified service
        session_id = str(uuid.uuid4())
        analysis_result = None
        job_matches = []
        
        async for event in claude_service.chat_stream_unified(message, context, session_id):
            if event.get("type") == "tool_execution_complete":
                tool_name = event.get("tool_name")
                result = event.get("result")
                
                if tool_name == "analyze_resume":
                    analysis_result = result
                elif tool_name == "match_jobs_with_resume":
                    job_matches = result.get('matches', [])
        
        if not analysis_result:
            raise HTTPException(status_code=400, detail="ç®€å†åˆ†æå¤±è´¥")
        
        return {
            "analysis": analysis_result,
            "job_matches": job_matches,
            "message_type": "resume_analysis",
            "total_matches": len(job_matches),
            "session_id": session_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Standalone resume analysis failed: {e}")
        raise HTTPException(status_code=500, detail=f"ç®€å†åˆ†æå¤±è´¥: {str(e)}")


@router.get("/skill-heatmap/{job_title}")
async def generate_skill_heatmap_standalone(job_title: str):
    """
    ç‹¬ç«‹æŠ€èƒ½çƒ­ç‚¹å›¾ç«¯ç‚¹ / Standalone skill heatmap endpoint
    å‘åå…¼å®¹ï¼Œå»ºè®®ä½¿ç”¨ç»Ÿä¸€èŠå¤©æ¥å£ / Backward compatibility, recommend using unified chat interface
    """
    try:
        logger.warning("Using deprecated standalone skill heatmap endpoint. Please use unified chat interface.")
        
        claude_service = get_claude_service()
        
        # ç›´æ¥è°ƒç”¨å·¥å…· / Direct tool call
        heatmap_data = await claude_service._tool_generate_skill_heatmap(job_title)
        
        return {
            "heatmap_data": heatmap_data,
            "message_type": "skill_heatmap",
            "job_title": job_title
        }
        
    except Exception as e:
        logger.error(f"Standalone skill heatmap generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"æŠ€èƒ½çƒ­ç‚¹å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")


@router.get("/market-insights")
async def get_market_insights_standalone(query: str):
    """
    ç‹¬ç«‹å¸‚åœºæ´å¯Ÿç«¯ç‚¹ / Standalone market insights endpoint
    å‘åå…¼å®¹ï¼Œå»ºè®®ä½¿ç”¨ç»Ÿä¸€èŠå¤©æ¥å£ / Backward compatibility, recommend using unified chat interface
    """
    try:
        logger.warning("Using deprecated standalone market insights endpoint. Please use unified chat interface.")
        
        claude_service = get_claude_service()
        
        # ç›´æ¥è°ƒç”¨å·¥å…· / Direct tool call
        insights_data = await claude_service._tool_get_market_insights(query)
        
        return {
            "insights": insights_data.get("content", ""),
            "message_type": "market_insights",
            "query": query
        }
        
    except Exception as e:
        logger.error(f"Standalone market insights failed: {e}")
        raise HTTPException(status_code=500, detail=f"å¸‚åœºæ´å¯Ÿå¤±è´¥: {str(e)}")


@router.get("/history/{user_id}")
async def get_chat_history(user_id: str, limit: int = 20):
    """
    è·å–èŠå¤©å†å² / Get chat history
    
    Args:
        user_id: ç”¨æˆ·ID / User ID
        limit: é™åˆ¶æ•°é‡ / Limit count
        
    Returns:
        èŠå¤©å†å² / Chat history
    """
    try:
        # TODO: ä»æ•°æ®åº“è·å–èŠå¤©å†å² / Get chat history from database
        # ç›®å‰è¿”å›ç©ºå†å²ï¼Œæœªæ¥å¯ä»¥ä¸SQLiteé›†æˆ / Currently return empty history, can integrate with SQLite in future
        
        return {
            "messages": [],
            "total_count": 0,
            "user_id": user_id,
            "sessions": []
        }
        
    except Exception as e:
        logger.error(f"Failed to get chat history: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–èŠå¤©å†å²å¤±è´¥: {str(e)}")


@router.delete("/session/{session_id}")
async def clear_session(session_id: str):
    """
    æ¸…é™¤ä¼šè¯çŠ¶æ€ / Clear session state
    
    Args:
        session_id: ä¼šè¯ID / Session ID
        
    Returns:
        æ¸…é™¤ç»“æœ / Clear result
    """
    try:
        claude_service = get_claude_service()
        
        # æ¸…é™¤ä¼šè¯çŠ¶æ€ / Clear session state
        if session_id in claude_service.session_state:
            del claude_service.session_state[session_id]
            
        return {
            "success": True,
            "message": f"Session {session_id} cleared",
            "session_id": session_id
        }
        
    except Exception as e:
        logger.error(f"Failed to clear session: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤ä¼šè¯å¤±è´¥: {str(e)}")


@router.get("/sessions")
async def list_sessions():
    """
    åˆ—å‡ºæ‰€æœ‰æ´»è·ƒä¼šè¯ / List all active sessions
    
    Returns:
        ä¼šè¯åˆ—è¡¨ / Session list
    """
    try:
        claude_service = get_claude_service()
        
        sessions = []
        for session_id, session_data in claude_service.session_state.items():
            sessions.append({
                "session_id": session_id,
                "messages_count": len(session_data.get("messages", [])),
                "has_resume_analysis": session_data.get("resume_analysis") is not None,
                "job_postings_count": len(session_data.get("job_postings", []))
            })
        
        return {
            "sessions": sessions,
            "total_sessions": len(sessions)
        }
        
    except Exception as e:
        logger.error(f"Failed to list sessions: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {str(e)}") 


@router.get("/token-usage")
async def get_token_usage(session_id: Optional[str] = None):
    """
    è·å–tokenä½¿ç”¨ç»Ÿè®¡ / Get token usage statistics
    
    Args:
        session_id: å¯é€‰çš„ä¼šè¯ID / Optional session ID
        
    Returns:
        Tokenä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯ / Token usage statistics
    """
    try:
        claude_service = get_claude_service()
        usage_stats = await claude_service.get_token_usage_stats(session_id)
        
        return {
            "status": "success",
            "data": usage_stats,
            "message": "Token usage statistics retrieved successfully"
        }
        
    except Exception as e:
        logger.error(f"Failed to get token usage: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve token usage statistics: {str(e)}"
        )


@router.post("/budget-alert")
async def check_budget_alert(daily_budget: float = 5.0):
    """
    æ£€æŸ¥é¢„ç®—è­¦å‘Š / Check budget alerts
    
    Args:
        daily_budget: æ¯æ—¥é¢„ç®—é™åˆ¶ (USD) / Daily budget limit in USD
        
    Returns:
        é¢„ç®—çŠ¶æ€ä¿¡æ¯ / Budget status information
    """
    try:
        claude_service = get_claude_service()
        budget_status = claude_service.token_tracker.check_budget_alert(daily_budget)
        
        return {
            "status": "success",
            "data": budget_status,
            "message": f"Budget status: {budget_status['alert_level']}"
        }
        
    except Exception as e:
        logger.error(f"Failed to check budget: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to check budget status: {str(e)}"
        ) 