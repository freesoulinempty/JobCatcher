"""
æ–‡ä»¶ä¸Šä¼ API / File upload API
"""

from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Request
from typing import Optional
import logging
import os
import uuid
import aiofiles
import base64

from ..models.user import ResumeUploadRequest
from ..services.claude_service import ClaudeService

logger = logging.getLogger(__name__)
router = APIRouter()


@router.post("/resume")
async def upload_resume(
    request: Request,
    file: UploadFile = File(...),
    user_id: str = Form(...)
):
    """
    ä¸Šä¼ ç®€å†æ–‡ä»¶ / Upload resume file
    
    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶ / Uploaded file
        user_id: ç”¨æˆ·ID / User ID
        
    Returns:
        ä¸Šä¼ ç»“æœå’Œæ–‡ä»¶ä¿¡æ¯ / Upload result and file info
    """
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹ / Validate file type
        allowed_types = ['application/pdf', 'text/plain', 'application/msword', 
                        'application/vnd.openxmlformats-officedocument.wordprocessingml.document']
        
        if file.content_type not in allowed_types:
            raise HTTPException(
                status_code=400, 
                detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.content_type}ã€‚æ”¯æŒçš„ç±»å‹: PDF, TXT, DOC, DOCX"
            )
        
        # éªŒè¯æ–‡ä»¶å¤§å° / Validate file size (32MB limit for Claude PDF support)
        max_size = 32 * 1024 * 1024  # 32MB - Claude PDFæ”¯æŒçš„æœ€å¤§å¤§å°
        file_content = await file.read()
        
        if len(file_content) > max_size:
            raise HTTPException(
                status_code=400,
                detail="æ–‡ä»¶å¤§å°è¶…è¿‡32MBé™åˆ¶"
            )
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å / Generate unique filename
        file_extension = os.path.splitext(file.filename)[1]
        unique_filename = f"{uuid.uuid4()}{file_extension}"
        file_path = f"uploads/{unique_filename}"
        
        # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨ / Ensure upload directory exists
        os.makedirs("uploads", exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶ / Save file
        async with aiofiles.open(file_path, 'wb') as f:
            await f.write(file_content)
        
        # ğŸ”¥ æ–°åŠŸèƒ½ï¼šä¸ºClaude 4å‡†å¤‡æ–‡æ¡£æ•°æ® / NEW: Prepare document data for Claude 4
        document_data = await _prepare_document_for_claude(file_path, file.content_type, file_content)
        
        # ğŸ”¥ READMEæ­¥éª¤2-3ï¼šClaude 4åˆ†æç®€å†å¹¶å‘é‡åŒ–å­˜å‚¨ / README steps 2-3: Claude 4 analysis and vectorization
        resume_id = None
        vector_stored = False
        try:
            resume_id = await _analyze_and_store_resume_vector(user_id, unique_filename, document_data, request)
            vector_stored = True
            logger.info(f"âœ… Resume vector stored successfully: {resume_id}")
        except Exception as e:
            logger.warning(f"âš ï¸ ç®€å†å‘é‡åŒ–å­˜å‚¨å¤±è´¥ï¼Œä½†æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {e}")
        
        logger.info(f"Resume uploaded successfully: {file.filename} for user {user_id}")
        
        return {
            "message": "ç®€å†ä¸Šä¼ æˆåŠŸ",
            "filename": file.filename,
            "unique_filename": unique_filename,
            "file_path": file_path,
            "content_type": file.content_type,
            "size": len(file_content),
            "user_id": user_id,
            # ğŸ”¥ å…³é”®ï¼šè¿”å›Claude 4å¯ä»¥ç›´æ¥ä½¿ç”¨çš„æ–‡æ¡£æ•°æ® / CRITICAL: Return document data for Claude 4
            "document_data": document_data,
            "claude_native_support": True,  # æ ‡è¯†æ”¯æŒClaudeåŸç”Ÿå¤„ç†
            "vector_stored": vector_stored,  # æ ‡è¯†æ˜¯å¦å·²å‘é‡åŒ–å­˜å‚¨
            "resume_id": resume_id  # ç®€å†å‘é‡IDï¼Œç”¨äºåç»­åŒ¹é…
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Resume upload failed: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")


async def _prepare_document_for_claude(file_path: str, content_type: str, file_content: bytes) -> dict:
    """
    ğŸ”¥ æ–°åŠŸèƒ½ï¼šä¸ºClaude 4å‡†å¤‡æ–‡æ¡£æ•°æ® / NEW: Prepare document data for Claude 4
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„ / File path
        content_type: æ–‡ä»¶ç±»å‹ / Content type
        file_content: æ–‡ä»¶å†…å®¹å­—èŠ‚ / File content bytes
        
    Returns:
        Claude 4å¯ä»¥ç›´æ¥ä½¿ç”¨çš„æ–‡æ¡£æ•°æ® / Document data that Claude 4 can use directly
    """
    try:
        if content_type == 'application/pdf':
            # ğŸ”¥ PDFæ–‡ä»¶ï¼šä½¿ç”¨Claude 4åŸç”ŸPDFæ”¯æŒ / PDF: Use Claude 4 native PDF support
            # å°†PDFæ–‡ä»¶ç¼–ç ä¸ºbase64ä¾›Claudeå¤„ç† / Encode PDF as base64 for Claude
            base64_content = base64.b64encode(file_content).decode('utf-8')
            
            return {
                "type": "document",
                "source": {
                    "type": "base64",
                    "media_type": "application/pdf",
                    "data": base64_content
                },
                "claude_format": "native_pdf"  # æ ‡è¯†ä½¿ç”¨ClaudeåŸç”ŸPDFå¤„ç†
            }
            
        elif content_type == 'text/plain':
            # æ–‡æœ¬æ–‡ä»¶ï¼šç›´æ¥è¯»å–å†…å®¹ / Text file: read content directly
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                text_content = await f.read()
            
            return {
                "type": "text",
                "content": text_content,
                "claude_format": "text"
            }
                
        elif content_type in ['application/msword', 
                             'application/vnd.openxmlformats-officedocument.wordprocessingml.document']:
            # Wordæ–‡æ¡£ï¼šæå–æ–‡æœ¬å†…å®¹ï¼ˆClaude 4æš‚ä¸æ”¯æŒåŸç”ŸWordå¤„ç†ï¼‰ / Word: extract text (Claude 4 doesn't support native Word yet)
            try:
                from docx import Document
                doc = Document(file_path)
                text_content = ""
                for paragraph in doc.paragraphs:
                    text_content += paragraph.text + "\n"
                
                return {
                    "type": "text",
                    "content": text_content,
                    "claude_format": "extracted_text",
                    "original_format": "docx"
                }
            except ImportError:
                raise HTTPException(status_code=500, detail="Wordæ–‡æ¡£å¤„ç†åº“æœªå®‰è£…")
        
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹")
            
    except Exception as e:
        logger.error(f"Document preparation for Claude failed: {e}")
        # å›é€€åˆ°æ–‡æœ¬æå– / Fallback to text extraction
        try:
            return await _fallback_text_extraction(file_path, content_type)
        except:
            raise HTTPException(status_code=500, detail=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")


async def _analyze_and_store_resume_vector(user_id: str, filename: str, document_data: dict, request) -> str:
    """
    ğŸ”¥ READMEæ­¥éª¤2-3ï¼šä½¿ç”¨Claude 4åˆ†æç®€å†å¹¶å‘é‡åŒ–å­˜å‚¨ / README steps 2-3: Analyze resume with Claude 4 and store vector
    """
    import uuid
    import json
    import time
    from datetime import datetime
    from ..database.connection import get_text_embedding
    
    try:
        # è·å–æ•°æ®åº“è¿æ¥ / Get database connections
        db_connections = request.app.state.db_connections
        resumes_collection = db_connections['resumes_collection']
        openai_client = db_connections['openai_client']
        claude_service = request.app.state.claude_service
        
        # ğŸ”¥ Claude 4ç®€å†åˆ†æ / Claude 4 resume analysis
        resume_analysis_prompt = """è¯·åˆ†æè¿™ä»½ç®€å†ï¼Œæå–ä»¥ä¸‹ç»“æ„åŒ–ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š

{
  "skills": ["æŠ€èƒ½1", "æŠ€èƒ½2", "æŠ€èƒ½3"],
  "experience_years": 3,
  "location": "Berlin",
  "education_level": "Bachelor",
  "languages": ["German", "English", "Chinese"],
  "summary": "ç®€å†æ¦‚è¿°"
}

é‡ç‚¹å…³æ³¨ï¼š
1. æŠ€æœ¯æŠ€èƒ½å’Œä¸“ä¸šæŠ€èƒ½
2. å·¥ä½œç»éªŒå¹´é™
3. åœ°ç†ä½ç½®åå¥½
4. æ•™è‚²æ°´å¹³
5. è¯­è¨€èƒ½åŠ›

è¯·è¿”å›æ ‡å‡†JSONæ ¼å¼ã€‚"""
        
        # ä½¿ç”¨Claude 4åˆ†æç®€å† / Use Claude 4 to analyze resume
        analysis_text = ""
        async for chunk in claude_service.chat_stream_unified(
            resume_analysis_prompt,
            context={"uploaded_document": document_data}
        ):
            if chunk.get("type") == "text":
                analysis_text += chunk.get("content", "")
        
        # è§£æåˆ†æç»“æœ / Parse analysis results
        try:
            # å°è¯•æå–JSON / Try to extract JSON
            import re
            json_match = re.search(r'\{.*\}', analysis_text, re.DOTALL)
            if json_match:
                resume_analysis = json.loads(json_match.group())
            else:
                raise ValueError("No JSON found in Claude response")
        except (json.JSONDecodeError, ValueError):
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼ / Use default values if parsing fails
            resume_analysis = {
                "skills": ["General"],
                "experience_years": 0,
                "location": "Germany",
                "education_level": "Unknown",
                "languages": ["German"],
                "summary": "ç®€å†åˆ†æå¤„ç†ä¸­"
            }
        
        # ğŸ”¥ æ„å»ºå‘é‡åŒ–æ–‡æœ¬ / Build vectorization text
        vectorization_text = _build_resume_vectorization_text(resume_analysis)
        
        # ğŸ”¥ ç”Ÿæˆå‘é‡ / Generate vector
        resume_vector = get_text_embedding(openai_client, vectorization_text)
        
        # ğŸ”¥ ç”Ÿæˆå”¯ä¸€ID / Generate unique ID
        resume_id = f"resume_{user_id}_{int(time.time())}"
        
        # ğŸ”¥ å­˜å‚¨åˆ°Chroma / Store in Chroma
        resumes_collection.add(
            embeddings=[resume_vector],
            documents=[vectorization_text],
            metadatas=[{
                "user_id": user_id,
                "resume_id": resume_id,
                "filename": filename,
                "skills": json.dumps(resume_analysis.get("skills", [])),
                "experience_years": resume_analysis.get("experience_years", 0),
                "location": resume_analysis.get("location", ""),
                "education_level": resume_analysis.get("education_level", ""),
                "languages": json.dumps(resume_analysis.get("languages", [])),
                "summary": resume_analysis.get("summary", ""),
                "created_at": datetime.now().isoformat(),
                "analysis_method": "claude4_native"
            }],
            ids=[resume_id]
        )
        
        logger.info(f"ğŸ“ Resume analysis and vector storage completed for user {user_id}")
        return resume_id
        
    except Exception as e:
        logger.error(f"âŒ Resume analysis and vector storage failed: {e}")
        raise


def _build_resume_vectorization_text(resume_analysis: dict) -> str:
    """
    ğŸ”¥ æ„å»ºç”¨äºå‘é‡åŒ–çš„ç®€å†æ–‡æœ¬ / Build resume text for vectorization
    æŒ‰ç…§READMEè¦æ±‚çš„æƒé‡ï¼šæŠ€èƒ½>ç»éªŒ>åœ°ç‚¹>è¯­è¨€>å­¦å†
    """
    components = []
    
    # æŠ€èƒ½ï¼ˆæœ€é«˜æƒé‡ï¼Œé‡å¤3æ¬¡å¢å¼ºé‡è¦æ€§ï¼‰/ Skills (highest weight, repeat 3 times)
    skills = resume_analysis.get("skills", [])
    if skills:
        skills_text = " ".join(skills)
        components.extend([f"Skills: {skills_text}"] * 3)
    
    # ç»éªŒï¼ˆé«˜æƒé‡ï¼Œé‡å¤2æ¬¡ï¼‰/ Experience (high weight, repeat 2 times)
    experience_years = resume_analysis.get("experience_years", 0)
    experience_text = f"Experience: {experience_years} years"
    components.extend([experience_text] * 2)
    
    # åœ°ç‚¹ï¼ˆä¸­ç­‰æƒé‡ï¼‰/ Location (medium weight)
    location = resume_analysis.get("location", "")
    if location:
        components.append(f"Location: {location}")
    
    # è¯­è¨€ï¼ˆä¸­ç­‰æƒé‡ï¼‰/ Languages (medium weight)
    languages = resume_analysis.get("languages", [])
    if languages:
        languages_text = " ".join(languages)
        components.append(f"Languages: {languages_text}")
    
    # æ•™è‚²ï¼ˆè¾ƒä½æƒé‡ï¼‰/ Education (lower weight)
    education = resume_analysis.get("education_level", "")
    if education:
        components.append(f"Education: {education}")
    
    # æ‘˜è¦ / Summary
    summary = resume_analysis.get("summary", "")
    if summary:
        components.append(f"Summary: {summary}")
    
    return " | ".join(components)


async def _fallback_text_extraction(file_path: str, content_type: str) -> dict:
    """
    å›é€€æ–‡æœ¬æå–æ–¹æ³• / Fallback text extraction method
    """
    try:
        if content_type == 'application/pdf':
            # PDFå›é€€æ–¹æ¡ˆ / PDF fallback
            try:
                import PyPDF2
                with open(file_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text()
                    
                    return {
                        "type": "text",
                        "content": text,
                        "claude_format": "extracted_text",
                        "original_format": "pdf"
                    }
            except ImportError:
                raise HTTPException(status_code=500, detail="PDFå¤„ç†åº“æœªå®‰è£…")
        
        # å…¶ä»–æƒ…å†µè¿”å›ç©ºå†…å®¹ / Return empty content for other cases
        return {
            "type": "text",
            "content": "",
            "claude_format": "empty",
            "error": "æ— æ³•å¤„ç†æ­¤æ–‡ä»¶ç±»å‹"
        }
        
    except Exception as e:
        logger.error(f"Fallback text extraction failed: {e}")
        return {
            "type": "text",
            "content": "",
            "claude_format": "empty",
            "error": str(e)
        }


@router.delete("/resume/{filename}")
async def delete_resume(filename: str, user_id: str):
    """
    åˆ é™¤ç®€å†æ–‡ä»¶ / Delete resume file
    
    Args:
        filename: æ–‡ä»¶å / Filename
        user_id: ç”¨æˆ·ID / User ID
        
    Returns:
        åˆ é™¤ç»“æœ / Deletion result
    """
    try:
        file_path = f"uploads/{filename}"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ / Check if file exists
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # åˆ é™¤æ–‡ä»¶ / Delete file
        os.remove(file_path)
        
        # TODO: ä»æ•°æ®åº“åˆ é™¤ç›¸å…³è®°å½• / Delete related records from database
        
        logger.info(f"Resume deleted: {filename} for user {user_id}")
        
        return {"message": "æ–‡ä»¶åˆ é™¤æˆåŠŸ", "filename": filename}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Resume deletion failed: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶åˆ é™¤å¤±è´¥: {str(e)}")


@router.get("/resume/{filename}")
async def get_resume_info(filename: str, user_id: str):
    """
    è·å–ç®€å†æ–‡ä»¶ä¿¡æ¯ / Get resume file info
    
    Args:
        filename: æ–‡ä»¶å / Filename
        user_id: ç”¨æˆ·ID / User ID
        
    Returns:
        æ–‡ä»¶ä¿¡æ¯ / File information
    """
    try:
        file_path = f"uploads/{filename}"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ / Check if file exists
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è·å–æ–‡ä»¶ä¿¡æ¯ / Get file info
        file_stat = os.stat(file_path)
        
        return {
            "filename": filename,
            "file_path": file_path,
            "size": file_stat.st_size,
            "created_time": file_stat.st_ctime,
            "modified_time": file_stat.st_mtime
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get resume info failed: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}") 


@router.get("/health")
async def health_check():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹ / Health check endpoint
    """
    return {"status": "ok", "message": "File upload service is running"} 