"""
Claude 4 Sonnet AIæœåŠ¡ / Claude 4 Sonnet AI service
ä¼˜åŒ–ç‰ˆæœ¬ - ç¬¦åˆå®˜æ–¹æ–‡æ¡£æ ‡å‡† / Optimized version - compliant with official documentation standards
"""

from typing import List, Dict, Any, Optional, AsyncGenerator
from anthropic import AsyncAnthropic
from ..core.config import settings
from ..models.job import JobPosting, JobMatch
from ..core.logging import get_logger
import json
from datetime import datetime

logger = get_logger("JobCatcher.claude")


class TokenUsageTracker:
    """Tokenä½¿ç”¨é‡è·Ÿè¸ªå™¨ / Token usage tracker"""
    
    def __init__(self):
        self.daily_usage = {}
        # Claude 4 Sonnetå®šä»· (USD per million tokens) / Claude 4 Sonnet pricing
        self.pricing = {
            "claude-sonnet-4-20250514": {"input": 3.0, "output": 15.0},
            "claude-opus-4-20250514": {"input": 15.0, "output": 75.0}
        }
    
    def log_usage(self, model: str, input_tokens: int, output_tokens: int, 
                  cache_creation_tokens: int = 0, cache_read_tokens: int = 0, 
                  web_search_requests: int = 0, session_id: str = None):
        """è®°å½•tokenä½¿ç”¨é‡ / Log token usage"""
        today = datetime.now().strftime("%Y-%m-%d")
        
        if today not in self.daily_usage:
            self.daily_usage[today] = {
                "sessions": {},
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_cache_creation_tokens": 0,
                "total_cache_read_tokens": 0,
                "total_web_search_requests": 0,
                "total_cost": 0.0,
                "cache_savings": 0.0,
                "model_usage": {}
            }
        
        daily_data = self.daily_usage[today]
        
        # ä¼šè¯çº§åˆ«ç»Ÿè®¡ / Session-level statistics
        if session_id:
            if session_id not in daily_data["sessions"]:
                daily_data["sessions"][session_id] = {
                    "input_tokens": 0,
                    "output_tokens": 0,
                    "cache_creation_tokens": 0,
                    "cache_read_tokens": 0,
                    "web_search_requests": 0,
                    "cost": 0.0,
                    "requests": 0
                }
            
            session_data = daily_data["sessions"][session_id]
            session_data["input_tokens"] += input_tokens
            session_data["output_tokens"] += output_tokens
            session_data["cache_creation_tokens"] += cache_creation_tokens
            session_data["cache_read_tokens"] += cache_read_tokens
            session_data["web_search_requests"] += web_search_requests
            session_data["requests"] += 1
            
        # å…¨å±€ç»Ÿè®¡ / Global statistics
        daily_data["total_input_tokens"] += input_tokens
        daily_data["total_output_tokens"] += output_tokens
        daily_data["total_cache_creation_tokens"] += cache_creation_tokens
        daily_data["total_cache_read_tokens"] += cache_read_tokens
        daily_data["total_web_search_requests"] += web_search_requests
        
        # æˆæœ¬è®¡ç®— / Cost calculation
        if model in self.pricing:
            pricing = self.pricing[model]
            cost = (input_tokens * pricing["input"] + output_tokens * pricing["output"]) / 1_000_000
            daily_data["total_cost"] += cost
            
            # ç¼“å­˜èŠ‚çœè®¡ç®— / Cache savings calculation
            cache_savings = cache_read_tokens * pricing["input"] * 0.9 / 1_000_000  # 90% discount for cache reads
            daily_data["cache_savings"] += cache_savings
            
            if session_id:
                daily_data["sessions"][session_id]["cost"] += cost
        
        # æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡ / Model usage statistics
        if model not in daily_data["model_usage"]:
            daily_data["model_usage"][model] = {
                "requests": 0,
                "input_tokens": 0,
                "output_tokens": 0,
                "cost": 0.0
            }
        
        daily_data["model_usage"][model]["requests"] += 1
        daily_data["model_usage"][model]["input_tokens"] += input_tokens
        daily_data["model_usage"][model]["output_tokens"] += output_tokens
        if model in self.pricing:
            daily_data["model_usage"][model]["cost"] += cost
        
        logger.info(f"Token usage - Model: {model}, Input: {input_tokens}, Output: {output_tokens}, "
                   f"Cache Created: {cache_creation_tokens}, Cache Read: {cache_read_tokens}, "
                   f"Web Searches: {web_search_requests}, Cost: ${cost:.4f}, Savings: ${cache_savings:.4f}")

    def get_daily_summary(self, date: str = None) -> Dict[str, Any]:
        """è·å–æ¯æ—¥ä½¿ç”¨ç»Ÿè®¡ / Get daily usage summary"""
        if not date:
            date = datetime.now().strftime("%Y-%m-%d")
        
        if date not in self.daily_usage:
            return {
                "date": date,
                "total_tokens": 0,
                "total_cost": 0.0,
                "cache_savings": 0.0,
                "requests": 0,
                "cache_hit_rate": 0.0
            }
        
        data = self.daily_usage[date]
        total_tokens = data["total_input_tokens"] + data["total_output_tokens"]
        total_requests = sum(session["requests"] for session in data["sessions"].values())
        cache_hit_rate = data["total_cache_read_tokens"] / max(data["total_input_tokens"], 1)
        
        return {
            "date": date,
            "total_tokens": total_tokens,
            "input_tokens": data["total_input_tokens"],
            "output_tokens": data["total_output_tokens"],
            "total_cost": data["total_cost"],
            "cache_savings": data["cache_savings"],
            "requests": total_requests,
            "cache_hit_rate": cache_hit_rate,
            "web_search_requests": data["total_web_search_requests"],
            "sessions": len(data["sessions"]),
            "model_usage": data["model_usage"]
        }

    def check_budget_alert(self, daily_budget: float = 5.0) -> Dict[str, Any]:
        """æ£€æŸ¥é¢„ç®—è­¦å‘Š / Check budget alert"""
        today = datetime.now().strftime("%Y-%m-%d")
        current_usage = self.get_daily_summary(today)
        used_amount = current_usage["total_cost"]
        percentage = (used_amount / daily_budget) * 100
        
        if percentage >= 100:
            alert_level = "exceeded"
        elif percentage >= 80:
            alert_level = "high"
        elif percentage >= 60:
            alert_level = "medium"
        else:
            alert_level = "low"
        
        return {
            "alert_level": alert_level,
            "used_amount": used_amount,
            "budget": daily_budget,
            "percentage": percentage,
            "remaining": max(0, daily_budget - used_amount)
        }


class ClaudeService:
    """Claude 4 Sonnet AIæœåŠ¡ç±» - ä¼˜åŒ–ç‰ˆæœ¬ / Claude 4 Sonnet AI service - optimized version"""
    
    def __init__(self):
        """åˆå§‹åŒ–ClaudeæœåŠ¡ / Initialize Claude service"""
        self.client = AsyncAnthropic(api_key=settings.anthropic_api_key)
        self.model = settings.claude_model
        self.token_tracker = TokenUsageTracker()
        
        # Sessionç¼“å­˜ç®¡ç† - å…³é”®ä¼˜åŒ– / Session cache management - key optimization
        self.session_system_cache = {}
        
        # ğŸ”¥ æ–°å¢ï¼šæ¶ˆæ¯å†å²ç®¡ç† / NEW: Message history management
        self.session_message_history = {}
        
        # ğŸ”¥ æ–°å¢ï¼šå›ºå®šå›ç­”æ¨¡æ¿ï¼ŒèŠ‚çœtoken / NEW: Fixed response templates to save tokens
        self.fixed_responses = {
            "welcome": {
                "pattern": r"^(hello|hi|ä½ å¥½|hallo|guten tag).*$",
                "response": """Hello! I'm JobCatcher, your AI-powered career assistant specializing in the German job market. I'm here to help you with:

ğŸ¯ **Resume Analysis & Optimization** - Upload your CV for detailed feedback
ğŸ” **Job Matching** - Find positions that match your background
ğŸ“Š **Skills Heatmap** - Discover trending skills in your field
ğŸ’¼ **Career Guidance** - Get advice for the German job market
ğŸŒ **Market Insights** - Current trends and salary information

How can I assist you today? You can upload your resume or ask me anything about your career in Germany!""",
                "skip_ai": True
            },
            "upload_complete": {
                "pattern": r"resume.*uploaded|file.*uploaded|cv.*uploaded",
                "response": """ğŸ“„ **Resume Upload Complete!**

Great! I've received your resume. Now you can ask me:

â€¢ ğŸ’¡ Analyze my resume strengths and weaknesses
â€¢ ğŸ¯ What skills am I missing for the job market?
â€¢ ğŸ”§ How can I improve my resume?
â€¢ ğŸ” Find matching jobs for me
â€¢ ğŸ“Š Generate skills heatmap for my target roles

What would you like to know about your career prospects in Germany?""",
                "skip_ai": True
            },
            "help": {
                "pattern": r"^(help|å¸®åŠ©|hilfe).*$",
                "response": """ğŸ¤– **JobCatcher Help**

I can help you with:

**Resume & Career Analysis:**
- Resume analysis and optimization
- Skills gap identification
- Career path recommendations

**Job Market Intelligence:**
- Find matching job opportunities
- Generate skills heatmaps for specific roles
- German job market trends and insights
- Salary information and expectations

**Career Guidance:**
- Interview preparation tips
- Cover letter guidance (German format)
- Professional networking advice
- Visa and work permit information

Just ask me anything or upload your resume to get started!""",
                "skip_ai": True
            }
        }
        
        # Tokené™åˆ¶é…ç½® / Token limit configuration
        self.max_tokens_limit = {
            "simple_response": 2500,
            "detailed_analysis": 4000,
            "complex_task": 6000
        }
        
        logger.info("Claude 4 service initialized - optimized version with prompt caching, context management, and fixed responses")

    def _initialize_session_context(self, session_id: str) -> str:
        """
        åˆå§‹åŒ–ä¼šè¯ä¸Šä¸‹æ–‡ - åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åˆ›å»ºç¼“å­˜
        Initialize session context - create cache only on first call
        """
        if session_id in self.session_system_cache:
            logger.info(f"â™»ï¸ Using existing cached system context for session: {session_id}")
            return self.session_system_cache[session_id]
        
        # ğŸ”¥ ä¼˜åŒ–ï¼šç®€åŒ–ç³»ç»Ÿæç¤ºè¯ï¼Œæ»¡è¶³1024tokenç¼“å­˜è¦æ±‚ä½†é¿å…é‡å¤ / OPTIMIZED: Simplified system prompt for cache efficiency
        system_prompt = """You are JobCatcher, an advanced AI career assistant powered by Claude 4 Sonnet, specializing in the German job market. Your mission is to help professionals find opportunities and advance their careers in Germany.

**Core Identity & Capabilities:**
- German job market expert with deep knowledge across all industries and regions
- Career counselor providing personalized guidance for job search, applications, and professional development
- Multilingual assistant supporting Chinese, German, and English communication

**Specialized Knowledge:**
- German employment landscape, salary trends, and hiring patterns across 16 BundeslÃ¤nder
- CV/resume optimization following German standards (DIN 5008, Lebenslauf format)
- Cover letter guidance with proper German business language
- Interview preparation covering German corporate culture and expectations
- Skills development pathways including certifications and Weiterbildung programs
- Professional networking strategies for German business communities
- Immigration requirements and visa guidance for international professionals

**Advanced Tools & Intelligence:**
- Real-time web search for current job market data, company information, and industry trends
- Native PDF document processing for resume analysis and optimization
- Vector-based job matching system for personalized recommendations
- Skills analysis and career planning with market-aligned suggestions

**Communication Style:**
- Natural, conversational tone with cultural sensitivity to German business practices
- Practical, actionable advice with specific next steps and timelines
- Empathetic guidance understanding job search challenges and career transitions
- Context-aware responses building on conversation history and user preferences

**Response Framework:**
Provide immediately useful guidance while encouraging deeper engagement. Include specific examples, real-world scenarios, and concrete action items. Reference relevant German companies, institutions, and resources when appropriate. Focus on delivering value through expertise in German job market dynamics and professional development strategies.

Your goal is to be the definitive career assistant for anyone seeking to advance professionally in Germany, whether German nationals, EU citizens, or international professionals navigating the Deutsche job market."""
        
        # ç¼“å­˜ç³»ç»Ÿæç¤º / Cache system prompt
        self.session_system_cache[session_id] = system_prompt
        logger.info(f"ğŸ”¥ Created and cached system context for session: {session_id} (approx. {len(system_prompt.split())} words)")
        
        return system_prompt

    def _manage_session_history(self, session_id: str, new_message: str, assistant_response: str = None):
        """
        ğŸ”¥ å…³é”®æ–°åŠŸèƒ½ï¼šç®¡ç†ä¼šè¯æ¶ˆæ¯å†å² / KEY NEW FEATURE: Manage session message history
        """
        if session_id not in self.session_message_history:
            self.session_message_history[session_id] = []
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ / Add user message
        self.session_message_history[session_id].append({
            "role": "user",
            "content": new_message
        })
        
        # å¦‚æœæœ‰åŠ©æ‰‹å“åº”ï¼Œæ·»åŠ åŠ©æ‰‹æ¶ˆæ¯ / If assistant response available, add assistant message
        if assistant_response:
            # ğŸ”¥ ä¼˜åŒ–ï¼šæˆªæ–­è¿‡é•¿çš„åŠ©æ‰‹å“åº”ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯ / Optimize: truncate long assistant responses
            truncated_response = self._truncate_long_response(assistant_response)
            self.session_message_history[session_id].append({
                "role": "assistant", 
                "content": truncated_response
            })
        
        # ğŸ”¥ ä¼˜åŒ–ï¼šé™åˆ¶å†å²é•¿åº¦ï¼Œä¿æŒæœ€è¿‘12æ¡æ¶ˆæ¯ï¼ˆ6è½®å¯¹è¯ï¼‰ / Optimize: limit to 12 messages (6 conversation turns)
        if len(self.session_message_history[session_id]) > 12:
            self.session_message_history[session_id] = self.session_message_history[session_id][-12:]
        
        # ğŸ”¥ è®¡ç®—å½“å‰å†å²çš„tokenä¼°ç®— / Calculate estimated tokens for current history
        estimated_tokens = self._estimate_history_tokens(self.session_message_history[session_id])
        
        logger.info(f"ğŸ“ Updated message history for session {session_id}: {len(self.session_message_history[session_id])} messages, ~{estimated_tokens} tokens")

    def _truncate_long_response(self, response: str, max_length: int = 800) -> str:
        """
        ğŸ”¥ æ–°åŠŸèƒ½ï¼šæˆªæ–­è¿‡é•¿çš„åŠ©æ‰‹å“åº”ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯ / NEW: Truncate long assistant responses
        """
        if len(response) <= max_length:
            return response
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­ / Try to truncate at sentence boundaries
        sentences = response.split('ã€‚')
        truncated = ""
        for sentence in sentences:
            if len(truncated + sentence + "ã€‚") <= max_length:
                truncated += sentence + "ã€‚"
            else:
                break
        
        if truncated:
            return truncated + "\n\n[Content truncated for context efficiency]"
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å¥å­è¾¹ç•Œï¼Œç›´æ¥æˆªæ–­ / Direct truncation if no sentence boundary found
            return response[:max_length] + "...\n\n[Content truncated for context efficiency]"

    def _estimate_history_tokens(self, history: List[Dict[str, str]]) -> int:
        """
        ğŸ”¥ æ–°åŠŸèƒ½ï¼šä¼°ç®—æ¶ˆæ¯å†å²çš„tokenæ•°é‡ / NEW: Estimate token count for message history
        """
        total_chars = 0
        for message in history:
            total_chars += len(message.get('content', ''))
        
        # ç²—ç•¥ä¼°ç®—ï¼šè‹±æ–‡~4å­—ç¬¦/tokenï¼Œä¸­æ–‡~2å­—ç¬¦/tokenï¼Œå–å¹³å‡3å­—ç¬¦/token
        # Rough estimate: English ~4 chars/token, Chinese ~2 chars/token, average 3 chars/token
        estimated_tokens = total_chars // 3
        return estimated_tokens

    def _compress_message_history(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        ğŸ”¥ æ–°åŠŸèƒ½ï¼šå‹ç¼©æ¶ˆæ¯å†å²ï¼Œä¿ç•™æœ€é‡è¦çš„ä¿¡æ¯ / NEW: Compress message history
        """
        if len(messages) <= 4:
            return messages
        
        # ä¿ç•™ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯å’Œæœ€å2è½®å¯¹è¯ / Keep first user message and last 2 conversation turns
        compressed = []
        
        # æ·»åŠ ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼ˆé€šå¸¸æ˜¯å…³é”®èƒŒæ™¯ä¿¡æ¯ï¼‰ / Add first message (usually key background info)
        if messages:
            compressed.append(messages[0])
        
        # æ·»åŠ æœ€è¿‘çš„4æ¡æ¶ˆæ¯ï¼ˆ2è½®å¯¹è¯ï¼‰ / Add last 4 messages (2 conversation turns)
        if len(messages) > 4:
            compressed.extend(messages[-4:])
        else:
            compressed.extend(messages[1:])  # é™¤äº†ç¬¬ä¸€æ¡çš„å…¶ä»–æ‰€æœ‰æ¶ˆæ¯
        
        return compressed

    def _build_message_history(self, session_id: str, current_message: str, context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ğŸ”¥ å…³é”®æ–°åŠŸèƒ½ï¼šæ„å»ºå®Œæ•´çš„æ¶ˆæ¯å†å²ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ / KEY NEW FEATURE: Build complete message history with context
        """
        messages = []
        
        # è·å–å†å²æ¶ˆæ¯ / Get historical messages
        if session_id and session_id in self.session_message_history:
            messages.extend(self.session_message_history[session_id])
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„å½“å‰æ¶ˆæ¯ / CRITICAL FIX: Build current message with context
        message_content = []
        
        # å¦‚æœæœ‰æ–‡ä»¶ä¸Šä¼ ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­ / If file upload context exists, add to message
        if context and (context.get('resume_uploaded') or context.get('uploaded_file')):
            if context.get('uploaded_file'):
                # ğŸ”¥ æ–°åŠŸèƒ½ï¼šå¤„ç†Claude 4åŸç”Ÿæ–‡æ¡£æ ¼å¼ / NEW: Handle Claude 4 native document format
                uploaded_file = context['uploaded_file']
                document_data = uploaded_file.get('document_data', {})
                
                if document_data.get('claude_format') == 'native_pdf':
                    # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨Claude 4åŸç”ŸPDFæ”¯æŒ / CRITICAL: Use Claude 4 native PDF support
                    message_content.append({
                        "type": "document",
                        "source": document_data["source"]
                    })
                    
                    # æ·»åŠ æ–‡æœ¬è¯´æ˜ / Add text description
                    message_content.append({
                        "type": "text",
                        "text": f"""I have uploaded my resume file "{uploaded_file.get('filename', 'resume')}". Please analyze this PDF document directly using your native PDF processing capabilities.

{current_message}"""
                    })
                    
                    logger.info(f"ğŸ“„ Using Claude 4 native PDF processing for session {session_id}: {uploaded_file.get('filename', 'unknown')}")
                    
                elif document_data.get('claude_format') in ['text', 'extracted_text']:
                    # æ–‡æœ¬æ ¼å¼æ–‡æ¡£ / Text format document
                    file_content = document_data.get('content', '')
                    message_content.append({
                        "type": "text",
                        "text": f"""I have uploaded my resume file "{uploaded_file.get('filename', 'resume')}". Here is the content:

{file_content}

{current_message}"""
                    })
                    
                    logger.info(f"ğŸ“„ Using extracted text for session {session_id}: {uploaded_file.get('filename', 'unknown')}")
                    
                else:
                    # å›é€€åˆ°æ—§æ ¼å¼ / Fallback to old format
                    file_content = uploaded_file.get('text_content', '')
                    if file_content:
                        message_content.append({
                            "type": "text",
                            "text": f"""I have uploaded my resume file "{uploaded_file.get('filename', 'resume')}". Here is the content:

{file_content}

{current_message}"""
                        })
                        logger.info(f"ğŸ“„ Using fallback text content for session {session_id}: {uploaded_file.get('filename', 'unknown')}")
                    else:
                        # å®Œå…¨å›é€€ / Complete fallback
                        message_content.append({
                            "type": "text", 
                            "text": current_message
                        })
                        logger.warning(f"âš ï¸ No document content available for session {session_id}")
            
            elif context.get('file_content'):
                # å‘åå…¼å®¹æ—§æ ¼å¼ / Backward compatibility with old format
                filename = context.get('filename', 'resume')
                file_content = context.get('file_content', '')
                
                message_content.append({
                    "type": "text",
                    "text": f"""I have uploaded my resume file "{filename}". Here is the content:

{file_content}

{current_message}"""
                })
                
                logger.info(f"ğŸ“„ Using legacy file content format for session {session_id}: {filename}")
            
            else:
                # æ²¡æœ‰æ–‡ä»¶å†…å®¹ / No file content
                message_content.append({
                    "type": "text",
                    "text": current_message
                })
                logger.warning(f"âš ï¸ Resume upload context exists but no content found for session {session_id}")
        
        # å¦‚æœæœ‰èŒä½æ•°æ®ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­ / If job data context exists, add to message
        elif context and context.get('job_postings'):
            job_count = len(context['job_postings'])
            message_content.append({
                "type": "text",
                "text": f"""{current_message}

Context: I have access to {job_count} job postings from the German job market that can be used for matching and analysis."""
            })
            
            logger.info(f"ğŸ’¼ Enhanced message with job postings context for session {session_id}: {job_count} jobs")
        
        else:
            # æ™®é€šæ¶ˆæ¯ / Regular message
            message_content.append({
                "type": "text",
                "text": current_message
            })
        
        # æ·»åŠ æ„å»ºçš„æ¶ˆæ¯å†…å®¹ / Add built message content
        messages.append({
            "role": "user",
            "content": message_content
        })
        
        logger.info(f"ğŸ”— Built message history for session {session_id}: {len(messages)} total messages, content_blocks: {len(message_content)}, context_enhanced: {bool(context and (context.get('resume_uploaded') or context.get('job_postings')))}")
        return messages

    def _should_use_web_search(self, message: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦webæœç´¢ / Determine if web search is needed"""
        message_lower = message.lower()
        
        # æ—¶é—´æ€§å…³é”®è¯ / Time-related keywords (å¤šè¯­è¨€æ”¯æŒ)
        time_keywords = [
            # ä¸­æ–‡
            '2025', 'æœ€æ–°', 'å½“å‰', 'ç°åœ¨', 'ä»Šå¹´', 'æ–°çš„', 'ç›®å‰', 'æœ€è¿‘',
            # è‹±æ–‡
            'current', 'latest', 'recent', 'now', 'new', 'today', 'this year',
            # å¾·è¯­ / German
            'aktuell', 'neueste', 'jetzt', 'derzeit', 'momentan', 'heute', 'dieses jahr',
            'neue', 'neuen', 'aktuelle', 'gegenwÃ¤rtig', 'heutig'
        ]
        
        # å¸‚åœº/è¡Œä¸šç›¸å…³å…³é”®è¯ / Market/industry-related keywords (å¤šè¯­è¨€æ”¯æŒ)
        market_keywords = [
            # ä¸­æ–‡
            'å¸‚åœº', 'è¡Œä¸š', 'è¶‹åŠ¿', 'å‘å±•', 'è–ªèµ„', 'å·¥èµ„', 'å°±ä¸š', 'å‰æ™¯', 
            'æ•°æ®', 'è¡Œæƒ…', 'çŠ¶å†µ', 'aiå¸‚åœº', 'aiè¡Œä¸š', 'äººå·¥æ™ºèƒ½',
            'æŠ€èƒ½', 'æŠ€èƒ½åˆ†æ', 'çƒ­é—¨æŠ€èƒ½', 'æŠ€èƒ½è¦æ±‚', 'æŠ€èƒ½çƒ­ç‚¹å›¾', 'æŠ€èƒ½çƒ­åŠ›å›¾',
            # è‹±æ–‡
            'market', 'industry', 'trend', 'development', 'salary', 'employment', 
            'prospect', 'data', 'statistics', 'situation', 'status', 'genai', 
            'artificial intelligence', 'ai market', 'tech industry',
            'skills', 'skill analysis', 'trending skills', 'skill requirements', 
            'skills heatmap', 'skill heatmap', 'hot skills', 'popular skills',
            # å¾·è¯­ / German
            'markt', 'branche', 'industrie', 'trend', 'entwicklung', 'gehalt', 
            'lohn', 'beschÃ¤ftigung', 'arbeitsmarkt', 'aussichten', 'perspektiven',
            'daten', 'statistiken', 'situation', 'ki-markt', 'technologie',
            'ki-branche', 'kÃ¼nstliche intelligenz', 'stellenmarkt', 'jobmarkt',
            'fÃ¤higkeiten', 'kompetenz', 'fertigkeiten', 'skill-analyse'
        ]
        
        # æ˜ç¡®æœç´¢è¯·æ±‚ / Explicit search requests (å¤šè¯­è¨€æ”¯æŒ)
        search_keywords = [
            # ä¸­æ–‡
            'æœç´¢', 'æŸ¥è¯¢', 'æŸ¥æ‰¾', 'äº†è§£',
            # è‹±æ–‡
            'search', 'find', 'lookup', 'information about', 'tell me about',
            # å¾·è¯­ / German
            'suchen', 'finden', 'suche nach', 'informationen Ã¼ber', 'erzÃ¤hl mir Ã¼ber'
        ]
        
        # é—®é¢˜å…³é”®è¯ / Question keywords (å¤šè¯­è¨€æ”¯æŒ)
        question_keywords = [
            # ä¸­æ–‡
            'å¦‚ä½•', 'æ€ä¹ˆæ ·', 'ä»€ä¹ˆ', 'æƒ…å†µ', 'çŠ¶æ€',
            # è‹±æ–‡
            'how', 'what', 'which', 'when', 'where', 'why',
            # å¾·è¯­ / German
            'wie', 'was', 'welche', 'wann', 'wo', 'warum', 'weshalb'
        ]
        
        has_time = any(keyword in message_lower for keyword in time_keywords)
        has_market = any(keyword in message_lower for keyword in market_keywords)
        explicit_search = any(keyword in message_lower for keyword in search_keywords)
        has_question = any(keyword in message_lower for keyword in question_keywords)
        
        # ğŸ”¥ ä¿®å¤ï¼šæ”¾å®½åˆ¤æ–­æ¡ä»¶ï¼Œé’ˆå¯¹å¾·å›½å¸‚åœºçš„å¤šè¯­è¨€æ”¯æŒ
        result = (
            (has_time and has_market) or  # åŒæ—¶åŒ…å«æ—¶é—´å’Œå¸‚åœº
            explicit_search or           # æ˜ç¡®çš„æœç´¢è¯·æ±‚
            (has_market and has_question) or  # å¸‚åœºç›¸å…³çš„é—®é¢˜
            ('genai' in message_lower) or     # GenAIç›¸å…³è¯é¢˜
            ('ki-markt' in message_lower) or  # å¾·è¯­AIå¸‚åœº
            ('kÃ¼nstliche intelligenz' in message_lower) or # å¾·è¯­äººå·¥æ™ºèƒ½
            ('deutschland' in message_lower) or # å¾·å›½ç›¸å…³
            (has_time and ('å¾·å›½' in message_lower or 'germany' in message_lower or 'deutschland' in message_lower))  # å…³äºå¾·å›½çš„æ—¶é—´ç›¸å…³é—®é¢˜
        )
        
        logger.info(f"ğŸ” Web search decision for '{message[:50]}...': time={has_time}, market={has_market}, explicit={explicit_search}, question={has_question}, result={result}")
        
        return result

    def _build_system_prompt_with_cache(self, session_id: str) -> List[Dict[str, Any]]:
        """æ„å»ºå¸¦ç¼“å­˜çš„ç³»ç»Ÿæç¤º / Build system prompt with cache"""
        
        system_content = self._initialize_session_context(session_id)
        
        return [
            {
                "type": "text", 
                "text": system_content,
                "cache_control": {"type": "ephemeral"}  # ç¼“å­˜æ§åˆ¶ / Cache control
            }
        ]

    def _build_tools_config(self, task_type: str, message: str) -> List[Dict[str, Any]]:
        """
        æ„å»ºå·¥å…·é…ç½® / Build tools configuration
        ğŸ”¥ ç¬¦åˆClaude 4æœ€æ–°æ–‡æ¡£æ ‡å‡†ï¼ŒåŒ…å«WebSearchå·¥å…· / Compliant with Claude 4 latest docs, includes WebSearch tools
        æ³¨æ„ï¼šArtifactsæ˜¯Claude 4çš„åŸç”ŸåŠŸèƒ½ï¼Œæ— éœ€æ˜¾å¼é…ç½® / Note: Artifacts is Claude 4 native feature, no explicit config needed
        """
        tools = []
        
        # ğŸ”¥ WebSearchå·¥å…· - ä½¿ç”¨å®˜æ–¹2025å¹´æ ‡å‡†é…ç½® / WebSearch tool - using official 2025 standard configuration
        needs_web_search = self._should_use_web_search(message) or task_type == "skill_heatmap_generation"
        
        if needs_web_search:
            tools.append({
                "type": "web_search_20250305",  # ğŸ”¥ å®˜æ–¹2025å¹´å·¥å…·ç±»å‹ / Official 2025 tool type
                "name": "web_search",
                "max_uses": 5,  # å¢åŠ ä½¿ç”¨æ¬¡æ•°æ”¯æŒå¤æ‚æŸ¥è¯¢ / Increase usage for complex queries
                "user_location": {
                    "type": "approximate", 
                    "country": "DE",  # ä¸“æ³¨å¾·å›½å¸‚åœº / Focus on German market
                    "timezone": "Europe/Berlin"
                }
            })
            logger.info(f"ğŸŒ WebSearchå·¥å…·å·²å¯ç”¨ï¼Œæœ€å¤§ä½¿ç”¨æ¬¡æ•°: 5, åŒºåŸŸ: å¾·å›½")
        
        # ğŸ”¥ é‡è¦è¯´æ˜ï¼šArtifactsæ— éœ€æ˜¾å¼é…ç½® / IMPORTANT: Artifacts doesn't need explicit configuration
        # Claude 4ä¼šåœ¨æ£€æµ‹åˆ°å¯è§†åŒ–è¯·æ±‚æ—¶è‡ªåŠ¨æ¿€æ´»ArtifactsåŠŸèƒ½ / Claude 4 auto-activates Artifacts when detecting visualization requests
        if (task_type in ["skill_heatmap_generation", "artifact_generation"] or
            any(keyword in message.lower() for keyword in [
                'visualization', 'chart', 'graph', 'heatmap', 'diagram', 'plot', 'artifact',
                'å¯è§†åŒ–', 'å›¾è¡¨', 'çƒ­åŠ›å›¾', 'å›¾å½¢', 'å›¾ç¤º', 'interactive', 'äº¤äº’å¼'
            ])):
            logger.info("ğŸ¨ ArtifactsåŠŸèƒ½å‡†å¤‡å°±ç»ª - Claude 4å°†æ ¹æ®éœ€è¦è‡ªåŠ¨æ¿€æ´»å¯è§†åŒ–åŠŸèƒ½")
        
        return tools

    def _determine_task_type(self, message: str, context: Optional[Dict[str, Any]] = None) -> str:
        """ç¡®å®šä»»åŠ¡ç±»å‹ / Determine task type"""
        message_lower = message.lower()
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸­çš„ä»»åŠ¡ç±»å‹ / Check task type in context
        if context and context.get('task_type') == 'skill_heatmap_generation':
            return "complex_task"
        
        if any(keyword in message_lower for keyword in [
            "heatmap", "çƒ­åŠ›å›¾", "çƒ­ç‚¹å›¾", "skills heatmap", "skill heatmap", "æŠ€èƒ½çƒ­åŠ›å›¾", 
            "skills analysis", "market trends", "æŠ€èƒ½åˆ†æ", "å¸‚åœºåˆ†æ", "comprehensive analysis",
            "skill map", "æŠ€èƒ½åœ°å›¾", "èƒ½åŠ›å›¾è°±"
        ]):
            return "complex_task"
        elif any(keyword in message_lower for keyword in [
            "analyze", "match", "resume", "cv", "è¯¦ç»†", "åˆ†æ", "åŒ¹é…", "ç®€å†", "job matching",
            "ç®€å†åˆ†æ", "èŒä½åŒ¹é…", "career advice", "èŒä¸šå»ºè®®"
        ]):
            return "detailed_analysis"
        else:
            return "simple_response"
    
    def _check_fixed_response(self, message: str) -> Optional[str]:
        """æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„å›ºå®šå›ç­” / Check for matching fixed responses"""
        import re
        
        message_clean = message.strip().lower()
        
        for response_type, response_data in self.fixed_responses.items():
            pattern = response_data["pattern"]
            if re.match(pattern, message_clean, re.IGNORECASE):
                logger.info(f"ğŸ”¥ ä½¿ç”¨å›ºå®šå›ç­”èŠ‚çœtoken: {response_type}")
                return response_data["response"]
        
        return None

    async def chat_stream_unified(
        self, 
        message: str, 
        context: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Claude 4ç»Ÿä¸€æµå¼èŠå¤©æ¥å£ - ç¬¦åˆå®˜æ–¹æ–‡æ¡£çš„å®ç°ï¼Œæ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†
        Claude 4 unified streaming chat interface - official documentation compliant with context management
        """
        try:
            logger.info(f"Starting Claude 4 optimized chat for session: {session_id or 'new'}")
            
            # ğŸ”¥ æ–°å¢ï¼šæ£€æŸ¥å›ºå®šå›ç­”ï¼ŒèŠ‚çœtoken / NEW: Check fixed responses to save tokens
            fixed_response = self._check_fixed_response(message)
            if fixed_response:
                # ç›´æ¥è¿”å›å›ºå®šå›ç­”ï¼Œä¸è°ƒç”¨AI
                yield {
                    "type": "start",
                    "session_id": session_id,
                    "model": "Fixed Response",
                    "task_type": "fixed_template",
                    "tools_available": 0,
                    "cache_optimized": True,
                    "token_saved": True
                }
                
                # æ¨¡æ‹Ÿæµå¼è¾“å‡º
                for char in fixed_response:
                    yield {
                        "type": "text_delta",
                        "content": char
                    }
                
                # æ›´æ–°ä¼šè¯å†å²
                if session_id:
                    self._manage_session_history(session_id, message, fixed_response)
                
                yield {
                    "type": "complete",
                    "session_id": session_id,
                    "response_length": len(fixed_response),
                    "tools_used": 0,
                    "cache_optimized": True,
                    "token_saved": True,
                    "cost_savings": "~$0.01-0.05"
                }
                return
            
            # æ£€æŸ¥é¢„ç®— / Check budget
            budget_status = self.token_tracker.check_budget_alert()
            if budget_status["alert_level"] == "exceeded":
                yield {
                    "type": "error",
                    "content": f"Daily budget exceeded: ${budget_status['used_amount']:.2f}. Please try again tomorrow."
                }
                return

            # é…ç½®è¯·æ±‚å‚æ•° / Configure request parameters
            task_type = self._determine_task_type(message, context)
            max_tokens = self.max_tokens_limit.get(task_type, 2500)
            tools_config = self._build_tools_config(task_type, message)
                                    
            # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šæ„å»ºå®Œæ•´çš„æ¶ˆæ¯å†å²ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ / CRITICAL FIX: Build complete message history with context
            messages = self._build_message_history(session_id, message, context)
            
            # ğŸ”¥ Tokenç›‘æ§ï¼šæ£€æŸ¥è¾“å…¥tokenæ•°é‡ / Token monitoring: check input token count
            estimated_input_tokens = self._estimate_history_tokens(messages)
            if estimated_input_tokens > 2000:  # é™ä½é˜ˆå€¼ï¼Œæ›´æ—©å‹ç¼©
                logger.warning(f"âš ï¸ High input token count for session {session_id}: ~{estimated_input_tokens} tokens")
                if estimated_input_tokens > 3000:  # æ›´æ¿€è¿›çš„å‹ç¼©ç­–ç•¥
                    # å¦‚æœtokenè¿‡å¤šï¼Œè¿›è¡Œå‹ç¼© / Compress if too many tokens
                    messages = self._compress_message_history(messages)
                    compressed_tokens = self._estimate_history_tokens(messages)
                    logger.info(f"ğŸ—œï¸ Compressed message history: {estimated_input_tokens} â†’ {compressed_tokens} tokens (saved {estimated_input_tokens - compressed_tokens})")
            
            # ğŸ”¥ æ ¸å¿ƒä¼˜åŒ–ï¼šæ ¹æ®å®˜æ–¹æ–‡æ¡£æ„å»ºAPIè¯·æ±‚ï¼ŒåŒ…å«æ¶ˆæ¯å†å² / Core optimization: build API request with message history
            request_config = {
                "model": self.model,
                "max_tokens": max_tokens,
                "messages": messages,  # ğŸ”¥ ä½¿ç”¨å®Œæ•´æ¶ˆæ¯å†å² / Use complete message history
            }
            
            # ğŸ”¥ ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ï¼šæ ¹æ®sessionæ˜¯å¦å­˜åœ¨å†³å®šç¼“å­˜ç±»å‹ / Optimize cache strategy
            if session_id and len(messages) > 1:  # åªæœ‰å¤šè½®å¯¹è¯æ‰ä½¿ç”¨ç¼“å­˜
                request_config["system"] = self._build_system_prompt_with_cache(session_id)
                logger.info(f"ğŸ”§ Using cached system prompt for session {session_id}")
            else:
                # æ–°sessionä½¿ç”¨ç®€åŒ–çš„ç³»ç»Ÿæç¤º / New session uses simplified system prompt
                system_content = self._initialize_session_context(session_id or "new")
                request_config["system"] = system_content
                logger.info(f"ğŸ’« Using fresh system prompt for new session")
            
            # åªæœ‰éœ€è¦æ—¶æ‰æ·»åŠ å·¥å…· / Add tools only when needed
            if tools_config:
                request_config["tools"] = tools_config
            
            # æ—¥å¿—è®°å½• / Logging
            web_search_enabled = any(tool.get('type') == 'web_search_20250305' for tool in tools_config)
            
            logger.info(f"Claude 4 optimized request: {max_tokens} max_tokens, {len(tools_config)} tools, "
                       f"task_type: {task_type}, message: '{message[:50]}...', "
                       f"cached_system: {'Yes' if session_id else 'No'}, history_length: {len(messages)}")
            logger.info(f"ğŸ› ï¸ Tools enabled: {[t.get('name', t.get('type')) for t in tools_config]}, Web search: {'âœ…' if web_search_enabled else 'âŒ'}")
            
            # ğŸ”¥ ä¼˜åŒ–ï¼šç«‹å³å‘é€èŒä½æ•°æ®ï¼Œé¿å…ç”¨æˆ·ç­‰å¾… / Optimized: Send job data immediately to avoid waiting
            if context and context.get('job_postings'):
                jobs_data = []
                for job in context['job_postings']:
                    # ç¡®ä¿descriptionå­—æ®µå®Œæ•´ / Ensure description field is complete
                    description = ""
                    if hasattr(job, 'description'):
                        description = job.description or ""
                    else:
                        description = job.get('description', job.get('full_description', ''))
                    
                    jobs_data.append({
                        "id": job.get('id', ''),
                        "title": job.get('title', ''),
                        "company_name": job.get('company_name', ''),
                        "location": job.get('location', ''),
                        "description": description,
                        "url": job.get('url', ''),
                        "work_type": job.get('work_type', 'Full-time'),
                        "salary": job.get('salary', ''),
                        "source": job.get('source', 'LinkedIn'),
                        "posted_time_ago": job.get('posted_time_ago', '')
                    })
                
                yield {
                    "type": "job_data",
                    "jobs": jobs_data,
                    "match_type": context.get('match_type', 'general'),
                    "total_jobs": len(jobs_data),
                    "session_id": session_id
                }
                
                logger.info(f"ğŸ“‹ âš¡ ç«‹å³å‘é€ {len(jobs_data)} ä¸ªèŒä½åˆ°å‰ç«¯ (æ— éœ€ç­‰å¾…botå®Œæˆ) / Immediately sent {len(jobs_data)} jobs to frontend (no need to wait for bot)")
            
            # å¼€å§‹æµå¼å“åº” / Start streaming response
            assistant_response = ""  # ğŸ”¥ æ”¶é›†åŠ©æ‰‹å“åº”ç”¨äºå†å²è®°å½• / Collect assistant response for history
            
            async with self.client.messages.stream(**request_config) as stream:
                text_content = ""
                tool_uses = []
                
                yield {
                    "type": "start",
                    "session_id": session_id,
                    "model": self.model,
                    "task_type": task_type,
                    "tools_available": len(tools_config),
                    "cache_optimized": bool(session_id),
                    "context_messages": len(messages)
                }
                
                async for event in stream:
                    try:
                        # å¤„ç†æµå¼äº‹ä»¶ / Handle streaming events
                        if event.type == 'content_block_start':
                            # å®‰å…¨åœ°å¤„ç†content_blockå¯¹è±¡ / Safely handle content_block object
                            content_block = getattr(event, 'content_block', {})
                            if hasattr(content_block, '__dict__'):
                                # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸ / Convert to serializable dict
                                content_block_dict = {
                                    "type": getattr(content_block, 'type', ''),
                                    "id": getattr(content_block, 'id', ''),
                                    "name": getattr(content_block, 'name', ''),
                                    "input": getattr(content_block, 'input', {}) if hasattr(content_block, 'input') else {}
                                }
                            else:
                                content_block_dict = content_block
                                
                            yield {
                                "type": "content_block_start",
                                "index": getattr(event, 'index', 0),
                                "content_block": content_block_dict
                            }
                        
                        elif event.type == 'content_block_delta':
                            # å¤„ç†æ–‡æœ¬å¢é‡ / Handle text delta
                            if hasattr(event.delta, 'text') and event.delta.text:
                                text_chunk = event.delta.text
                                text_content += text_chunk
                                assistant_response += text_chunk  # ğŸ”¥ æ”¶é›†å“åº” / Collect response
                                yield {
                                    "type": "text_delta",
                                    "content": text_chunk
                                }
                        
                        elif event.type == 'content_block_stop':
                            yield {
                                "type": "content_block_stop",
                                "index": getattr(event, 'index', 0)
                            }
                        
                        elif event.type == 'message_stop':
                            # æµå¼ç»“æŸï¼Œä½¿ç”¨final_textå’Œusageä¿¡æ¯
                            yield {
                                "type": "message_stop"
                            }
                    
                    except Exception as event_error:
                        logger.error(f"Error processing stream event: {event_error}")
                        continue
                
                # ğŸ”¥ ä¼˜åŒ–ï¼šåœ¨æµå¼å“åº”å¼€å§‹æ—¶å°±å‘é€èŒä½æ•°æ®ï¼Œæ— éœ€ç­‰å¾…botå®Œæˆ / Optimized: Send job data at start of streaming
                # é¿å…ç”¨æˆ·ç­‰å¾…ï¼Œæå‡ç”¨æˆ·ä½“éªŒ / Avoid user waiting, improve UX
                
                # ğŸ”¥ å…³é”®ï¼šæ›´æ–°ä¼šè¯å†å² / CRITICAL: Update session history
                if session_id and assistant_response.strip():
                    self._manage_session_history(session_id, message, assistant_response.strip())
                
                # è·å–æœ€ç»ˆä½¿ç”¨ç»Ÿè®¡ / Get final usage statistics
                try:
                    final_message = await stream.get_final_message()
                    if hasattr(final_message, 'usage') and final_message.usage:
                        usage = final_message.usage
                        
                        # ğŸ”¥ å…³é”®ï¼šæ­£ç¡®è§£æå®˜æ–¹APIå“åº” / Key: correctly parse official API response
                        input_tokens = getattr(usage, 'input_tokens', 0)
                        output_tokens = getattr(usage, 'output_tokens', 0)
                        cache_creation_tokens = getattr(usage, 'cache_creation_input_tokens', 0)
                        cache_read_tokens = getattr(usage, 'cache_read_input_tokens', 0)
                        
                        # Webæœç´¢ä½¿ç”¨é‡ / Web search usage
                        web_search_requests = 0
                        if hasattr(usage, 'server_tool_use') and usage.server_tool_use:
                            web_search_requests = getattr(usage.server_tool_use, 'web_search_requests', 0)
                        
                        # è®°å½•tokenä½¿ç”¨ / Log token usage
                        self.token_tracker.log_usage(
                            model=self.model,
                            input_tokens=input_tokens,
                            output_tokens=output_tokens,
                            cache_creation_tokens=cache_creation_tokens,
                            cache_read_tokens=cache_read_tokens,
                            web_search_requests=web_search_requests,
                            session_id=session_id
                        )
                        
                        # è®°å½•é‡è¦äº‹ä»¶ / Log important events
                        if cache_read_tokens > 0:
                            logger.info(f"âœ… Claude 4 cache hit! Read {cache_read_tokens} tokens, saved ${cache_read_tokens * 3.0 * 0.9 / 1_000_000:.4f}")
                        if cache_creation_tokens > 0:
                            logger.info(f"ğŸ“ Claude 4 cache created! Wrote {cache_creation_tokens} tokens")
                        if web_search_requests > 0:
                            logger.info(f"ğŸ” Claude 4 web search used {web_search_requests} times")
                    else:
                        logger.warning("No usage information available from Claude 4 response")
                        
                except Exception as usage_error:
                    logger.error(f"Failed to get usage statistics: {usage_error}")

                # å®Œæˆå“åº” / Complete response
                yield {
                    "type": "complete",
                    "session_id": session_id,
                    "response_length": len(text_content),
                    "tools_used": len(tool_uses),
                    "cache_optimized": bool(session_id),
                    "context_preserved": bool(session_id and assistant_response.strip())
                }
                    
        except Exception as e:
            logger.error(f"Claude 4 streaming failed: {e}")
            yield {
                "type": "error",
                "content": f"AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚é”™è¯¯: {str(e)}"
            }

    # ğŸ”¥ æ–°å¢ï¼šæ¸…é™¤ä¼šè¯å†å²æ–¹æ³• / NEW: Clear session history method
    def clear_session_history(self, session_id: str):
        """æ¸…é™¤æŒ‡å®šä¼šè¯çš„å†å²è®°å½• / Clear history for specified session"""
        if session_id in self.session_message_history:
            del self.session_message_history[session_id]
            logger.info(f"ğŸ—‘ï¸ Cleared message history for session: {session_id}")
        
        if session_id in self.session_system_cache:
            del self.session_system_cache[session_id]
            logger.info(f"ğŸ—‘ï¸ Cleared system cache for session: {session_id}")

    # ğŸ”¥ æ–°å¢ï¼šè·å–ä¼šè¯å†å²æ–¹æ³• / NEW: Get session history method
    def get_session_history(self, session_id: str) -> List[Dict[str, str]]:
        """è·å–æŒ‡å®šä¼šè¯çš„å†å²è®°å½• / Get history for specified session"""
        return self.session_message_history.get(session_id, [])
    
    async def get_token_usage_stats(self, session_id: str = None) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ / Get usage statistics"""
        daily_stats = self.token_tracker.get_daily_summary()
        budget_status = self.token_tracker.check_budget_alert()
        
        # ç¡®ä¿åŒ…å«å½“å‰sessionçš„ç»Ÿè®¡æ•°æ® / Ensure current session stats are included
        if session_id and daily_stats.get('total_tokens', 0) == 0:
            # å¦‚æœæ²¡æœ‰ç»Ÿè®¡æ•°æ®ï¼Œå°è¯•ä»å†…å­˜ä¸­è·å– / If no stats, try to get from memory
            logger.info(f"No daily stats found, using session data for {session_id}")
        
        return {
            "daily_usage": daily_stats,
            "budget_status": budget_status,
            "optimization_benefits": {
                "daily_cache_savings": daily_stats.get("cache_savings", 0),
                "cache_hit_rate": daily_stats.get("cache_hit_rate", 0),
                "web_search_requests": daily_stats.get("web_search_requests", 0),
                "estimated_monthly_savings": daily_stats.get("cache_savings", 0) * 30
            },
            "claude4_features": {
                "prompt_caching": "1h TTL with 90% cost reduction",
                "web_search": "Native integration with geo-location",
                "streaming": "Real-time response delivery",
                "session_management": "Automatic context preservation",
                "message_history": f"Active sessions: {len(self.session_message_history)}"
            }
        }
    
    # ç®€åŒ–çš„å‘åå…¼å®¹æ–¹æ³• / Simplified backward compatibility methods
    async def analyze_resume(self, file_content: str, filename: str) -> Dict[str, Any]:
        """ç®€åŒ–çš„ç®€å†åˆ†æ / Simplified resume analysis"""
        try:
            prompt = f"Please analyze this resume file '{filename}' and provide structured analysis."
            
            result_content = ""
            async for chunk in self.chat_stream_unified(
                prompt, 
                context={"file_content": file_content, "filename": filename}
            ):
                if chunk.get("type") == "text":
                    result_content += chunk.get("content", "")
            
            return {
                "analysis_text": result_content,
                "skills": [],
                "experience_years": 0,
                "education_level": "Unknown",
                "languages": [],
                "summary": "Resume analyzed via Claude 4"
            }
            
        except Exception as e:
            logger.error(f"Resume analysis failed: {e}")
            return {"error": str(e), "summary": "Analysis failed"}

    async def match_jobs_with_resume(
        self, 
        resume_analysis: Dict[str, Any], 
        job_postings: List[JobPosting]
    ) -> List[JobMatch]:
        """
        ğŸ”¥ READMEæµç¨‹å®ç°ï¼šClaude 4æ·±åº¦åˆ†æç®€å†å’ŒèŒä½åŒ¹é… / README Flow: Claude 4 deep analysis for resume-job matching
        ä¸¥æ ¼æŒ‰ç…§READMEç¬¬5-6æ­¥ï¼šæ‹¼æ¥prompt â†’ Claudeåˆ†ææ’åº
        """
        try:
            # æ„å»ºä¸“é—¨çš„èŒä½æ¨èæç¤ºè¯ï¼ˆç§»å‡ºç³»ç»Ÿæç¤ºé¿å…é‡å¤ï¼‰/ Build dedicated job recommendation prompt
            job_matching_prompt = self._build_job_matching_prompt(resume_analysis, job_postings)
            
            # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨Claude 4è¿›è¡Œæ·±åº¦åˆ†æå’Œæ’åº / KEY: Use Claude 4 for deep analysis and ranking
            claude_analysis = ""
            async for chunk in self.chat_stream_unified(
                job_matching_prompt,
                context={
                    "task_type": "job_matching",
                    "resume_analysis": resume_analysis,
                    "job_count": len(job_postings)
                }
            ):
                if chunk.get("type") == "text":
                    claude_analysis += chunk.get("content", "")
            
            # è§£æClaude 4çš„åˆ†æç»“æœä¸ºJobMatchå¯¹è±¡ / Parse Claude 4 analysis into JobMatch objects
            job_matches = self._parse_claude_job_analysis(claude_analysis, job_postings)
            
            logger.info(f"âœ… Claude 4 job matching completed: {len(job_matches)} matches generated")
            return job_matches
            
        except Exception as e:
            logger.error(f"âŒ Job matching failed: {e}")
            return []

    def _build_job_matching_prompt(self, resume_analysis: Dict[str, Any], job_postings: List[JobPosting]) -> str:
        """
        ğŸ”¥ æ„å»ºèŒä½åŒ¹é…ä¸“ç”¨æç¤ºè¯ / Build dedicated job matching prompt
        æŒ‰ç…§READMEè¦æ±‚çš„åŒ¹é…è¦ç´ æƒé‡ï¼šæŠ€èƒ½>ç»éªŒ>åœ°ç‚¹>è¯­è¨€>å­¦å†
        """
        # æå–ç®€å†å…³é”®ä¿¡æ¯ / Extract key resume information
        skills = ', '.join(resume_analysis.get('skills', []))
        experience_years = resume_analysis.get('experience_years', 0)
        location = resume_analysis.get('location', 'æœªæŒ‡å®š')
        education = resume_analysis.get('education_level', 'æœªçŸ¥')
        languages = ', '.join(resume_analysis.get('languages', []))
        
        prompt = f"""ä½œä¸ºä¸“ä¸šçš„å¾·å›½å°±ä¸šå¸‚åœºé¡¾é—®ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ç®€å†åˆ†æ{len(job_postings)}ä¸ªèŒä½çš„åŒ¹é…åº¦å¹¶æ’åºã€‚

**ç®€å†æ¦‚è¦ï¼š**
- æŠ€èƒ½ï¼š{skills}
- å·¥ä½œç»éªŒï¼š{experience_years}å¹´
- æœŸæœ›åœ°ç‚¹ï¼š{location}
- æ•™è‚²èƒŒæ™¯ï¼š{education}
- è¯­è¨€èƒ½åŠ›ï¼š{languages}

**åŒ¹é…æƒé‡ï¼ˆé‡è¦æ€§æ’åºï¼‰ï¼š**
1. æŠ€èƒ½åŒ¹é…ï¼ˆ40%ï¼‰- é‡è¦
2. ç»éªŒåŒ¹é…ï¼ˆ30%ï¼‰- é‡è¦  
3. åœ°ç‚¹åŒ¹é…ï¼ˆ15%ï¼‰- ä¸­ç­‰é‡è¦
4. è¯­è¨€åŒ¹é…ï¼ˆ10%ï¼‰- ä¸­ç­‰é‡è¦
5. æ•™è‚²åŒ¹é…ï¼ˆ5%ï¼‰- è¾ƒä½é‡è¦

**å€™é€‰èŒä½ï¼š**
"""
        
        # æ·»åŠ æ‰€æœ‰èŒä½ä¿¡æ¯ / Add all job information
        for i, job in enumerate(job_postings, 1):
            job_desc = job.description[:300] if len(job.description) > 300 else job.description
            prompt += f"""
{i}. ã€{job.title}ã€‘- {job.company_name}
   åœ°ç‚¹ï¼š{job.location}
   å·¥ä½œç±»å‹ï¼š{getattr(job, 'job_type', 'æœªçŸ¥')}
   èŒä½æè¿°ï¼š{job_desc}...
"""

        prompt += """
**è¦æ±‚è¾“å‡ºJSONæ ¼å¼ï¼š**
è¯·ä¸ºæ¯ä¸ªèŒä½æä¾›è¯¦ç»†åˆ†æï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONç»“æ„è¿”å›ï¼š

```json
{
  "analysis_summary": "æ€»ä½“åˆ†ææ¦‚è¿°",
  "job_matches": [
    {
      "job_index": 1,
      "job_title": "èŒä½æ ‡é¢˜",
      "company_name": "å…¬å¸åç§°",
      "match_score": 85,
      "match_level": "excellent",
      "match_reasons": [
        "æŠ€èƒ½é«˜åº¦åŒ¹é…ï¼šPython, Reactç­‰æ ¸å¿ƒæŠ€èƒ½å®Œå…¨ç¬¦åˆ",
        "ç»éªŒå¹´é™åŒ¹é…ï¼šè¦æ±‚3-5å¹´ï¼Œå€™é€‰äººæœ‰4å¹´ç»éªŒ"
      ],
      "skill_matches": ["Python", "React", "Node.js"],
      "missing_skills": ["TypeScript", "Docker"],
      "location_match": true,
      "experience_match": true,
      "improvement_suggestions": [
        "å»ºè®®å­¦ä¹ TypeScriptæå‡å‰ç«¯å¼€å‘èƒ½åŠ›",
        "è¡¥å……Dockerå®¹å™¨åŒ–æŠ€èƒ½"
      ]
    }
  ]
}
```

**è¯„åˆ†æ ‡å‡†ï¼š**
- 90-100åˆ†ï¼šexcellentï¼ˆå®Œç¾åŒ¹é…ï¼‰
- 75-89åˆ†ï¼šgoodï¼ˆè‰¯å¥½åŒ¹é…ï¼‰
- 60-74åˆ†ï¼šfairï¼ˆä¸€èˆ¬åŒ¹é…ï¼‰
- 40-59åˆ†ï¼špoorï¼ˆè¾ƒä½åŒ¹é…ï¼‰
- 0-39åˆ†ï¼švery_poorï¼ˆä¸åŒ¹é…ï¼‰

è¯·æŒ‰åŒ¹é…åˆ†æ•°ä»é«˜åˆ°ä½æ’åºï¼Œæä¾›è¯¦ç»†çš„åŒ¹é…åŸå› å’Œæ”¹è¿›å»ºè®®ã€‚"""
        
        return prompt

    def _parse_claude_job_analysis(self, claude_analysis: str, job_postings: List[JobPosting]) -> List[JobMatch]:
        """
        ğŸ”¥ è§£æClaude 4çš„èŒä½åˆ†æç»“æœ / Parse Claude 4 job analysis results
        """
        import json
        import re
        
        try:
            # å°è¯•æå–JSONéƒ¨åˆ† / Try to extract JSON part
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', claude_analysis, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                analysis_data = json.loads(json_str)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•ç›´æ¥è§£æ / If no JSON format found, try direct parsing
                analysis_data = json.loads(claude_analysis)
            
            matches = []
            job_matches_data = analysis_data.get('job_matches', [])
            
            for match_data in job_matches_data:
                job_index = match_data.get('job_index', 1) - 1  # è½¬æ¢ä¸º0åŸºç´¢å¼•
                
                if 0 <= job_index < len(job_postings):
                    job = job_postings[job_index]
                    
                    # åˆ›å»ºJobMatchå¯¹è±¡ / Create JobMatch object
                    job_match = JobMatch(
                    job_posting=job,
                        matching_score=match_data.get('match_score', 50),
                        matching_reasons=match_data.get('match_reasons', []),
                        missing_skills=match_data.get('missing_skills', []),
                        recommended_improvements=match_data.get('improvement_suggestions', [])
                    )
                    
                    # æ·»åŠ é¢å¤–çš„åŒ¹é…ä¿¡æ¯ / Add additional matching info
                    job_match.skill_matches = match_data.get('skill_matches', [])
                    job_match.match_level = match_data.get('match_level', 'fair')
                    job_match.location_match = match_data.get('location_match', False)
                    job_match.experience_match = match_data.get('experience_match', False)
                    
                    matches.append(job_match)
            
            # æŒ‰åŒ¹é…åˆ†æ•°æ’åº / Sort by matching score
            matches.sort(key=lambda x: x.matching_score, reverse=True)
            
            logger.info(f"âœ… Parsed {len(matches)} job matches from Claude analysis")
            return matches
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Failed to parse Claude job analysis JSON: {e}")
            # åˆ›å»ºé»˜è®¤åŒ¹é…ç»“æœ / Create default match results
            return self._create_default_matches(job_postings)
        except Exception as e:
            logger.error(f"âŒ Error parsing Claude job analysis: {e}")
            return self._create_default_matches(job_postings)

    def _create_default_matches(self, job_postings: List[JobPosting]) -> List[JobMatch]:
        """åˆ›å»ºé»˜è®¤çš„èŒä½åŒ¹é…ç»“æœ / Create default job match results"""
        matches = []
        for i, job in enumerate(job_postings[:10]):  # é™åˆ¶å‰10ä¸ª
            matches.append(JobMatch(
                job_posting=job,
                matching_score=max(70 - i * 5, 40),  # é€’å‡åˆ†æ•°
                matching_reasons=["è‡ªåŠ¨åˆ†æ", "åŸºç¡€åŒ¹é…"],
                missing_skills=[],
                recommended_improvements=["è¯·ä¸Šä¼ è¯¦ç»†ç®€å†è·å¾—ç²¾å‡†åˆ†æ"]
            ))
        return matches

    async def generate_skill_heatmap_data(self, job_title: str) -> Dict[str, Any]:
        """
        ğŸ”¥ READMEæµç¨‹å®ç°ï¼šæŠ€èƒ½çƒ­ç‚¹å›¾ç”Ÿæˆ
        ä½¿ç”¨Claude 4åŸç”ŸWebSearchæœç´¢å²—ä½çƒ­ç‚¹æŠ€èƒ½å¹¶è¿›è¡Œæ·±åº¦æ€è€ƒï¼Œç„¶åä½¿ç”¨Artifactså·¥å…·ç”ŸæˆæŠ€èƒ½çƒ­ç‚¹å›¾å¯è§†åŒ–
        ç¬¦åˆClaude 4æœ€æ–°æ–‡æ¡£æ ‡å‡†ï¼Œæ­£ç¡®è°ƒç”¨å›¾ç”Ÿæˆå·¥å…·
        """
        try:
            # ğŸ”¥ ä¿®å¤ï¼šæ„å»ºæ˜ç¡®è§¦å‘Artifactsçš„æç¤ºè¯ï¼Œæ ¹æ®æœ€æ–°æ–‡æ¡£æ ‡å‡†
            prompt = f"""I need you to create an interactive skills heatmap for "{job_title}" positions. Please follow these steps:

**Step 1: Search Latest Market Data**
Use WebSearch to find information about "{job_title}" skills requirements and market trends in Germany for 2025:
- German job market demand for {job_title} skills
- 2025 trending skills for {job_title} positions  
- Salary and skill requirements for {job_title}
- Emerging technologies and trends in {job_title} field

**Step 2: Create Interactive Heatmap Visualization**
Based on the search results, create a complete interactive skills heatmap as an HTML artifact with:

1. **HTML structure** with skill categories
2. **CSS styling** with color-coded skill importance (dark = high demand, light = low demand)
3. **JavaScript interactivity** for:
   - Hover effects showing skill details
   - Click functionality to show learning resources
   - Responsive design for mobile devices
   - Smooth animations and transitions

4. **Skill Categories to include:**
   - Technical Skills (programming languages, frameworks, tools)
   - Soft Skills (communication, collaboration, leadership)  
   - Industry Knowledge (certifications, domain expertise)
   - Emerging Skills (AI/ML, cloud computing, data analysis)

5. **Interactive Features:**
   - Each skill block shows importance score (0-100)
   - Color intensity represents market demand
   - Tooltips with detailed descriptions
   - Learning recommendations on click
   - Professional development pathways

Please create this as a complete, self-contained HTML artifact that I can download and use. Make it visually appealing with modern UI design and ensure it works on both desktop and mobile devices.

The heatmap should be based on current German market data for {job_title} positions."""
            
            # ğŸ”¥ ä½¿ç”¨å¸¦WebSearch + Artifactsçš„unifiedæ¥å£
            result_content = ""
            websearch_used = False
            artifacts_generated = False
            
            logger.info(f"ğŸ”¥ å¼€å§‹ç”Ÿæˆ {job_title} äº¤äº’å¼æŠ€èƒ½çƒ­ç‚¹å›¾ï¼Œä½¿ç”¨WebSearch + Artifacts")
            
            async for chunk in self.chat_stream_unified(
                prompt,
                context={
                    "task_type": "skill_heatmap_generation", 
                    "force_websearch": True,
                    "enable_artifacts": True,
                    "visualization_request": True
                },
                session_id=f"heatmap_{job_title.replace(' ', '_')}"
            ):
                if chunk.get("type") == "text_delta":
                    result_content += chunk.get("content", "")
                elif chunk.get("type") == "text":
                    result_content += chunk.get("content", "")
                elif chunk.get("type") == "content_block_start":
                    # æ£€æµ‹Artifactsç”Ÿæˆ
                    content_block = chunk.get("content_block", {})
                    if content_block.get("type") == "tool_use" and content_block.get("name") == "artifacts":
                        artifacts_generated = True
                        logger.info("ğŸ¨ Artifactså·¥å…·æ­£åœ¨ç”Ÿæˆäº¤äº’å¼æŠ€èƒ½çƒ­ç‚¹å›¾...")
                elif chunk.get("type") == "tool_use":
                    if chunk.get("tool_name") == "web_search":
                        websearch_used = True
                        logger.info("ğŸŒ WebSearchæ­£åœ¨æœç´¢æœ€æ–°æŠ€èƒ½å¸‚åœºæ•°æ®...")
            
            # ğŸ”¥ ç¬¬äºŒæ­¥ï¼šå¦‚æœæ²¡æœ‰ç”ŸæˆArtifactsï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆåˆ›å»ºå¯è§†åŒ–æ•°æ®
            if not artifacts_generated:
                logger.info("ğŸ”„ Artifactsæœªè‡ªåŠ¨ç”Ÿæˆï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆåˆ›å»ºå¯è§†åŒ–æ•°æ®")
                
                # è°ƒç”¨ä¸“é—¨çš„Artifactsç”Ÿæˆ
                artifacts_prompt = f"""åŸºäºå‰é¢æœç´¢åˆ°çš„{job_title}æŠ€èƒ½æ•°æ®ï¼Œè¯·åˆ›å»ºä¸€ä¸ªäº¤äº’å¼æŠ€èƒ½çƒ­ç‚¹å›¾Artifactã€‚

è¦æ±‚ï¼š
1. ä½¿ç”¨HTML + CSS + JavaScriptåˆ›å»ºäº¤äº’å¼çƒ­åŠ›å›¾
2. æŠ€èƒ½æŒ‰ç±»åˆ«åˆ†ç»„ï¼ˆæŠ€æœ¯æŠ€èƒ½ã€è½¯æŠ€èƒ½ã€è¡Œä¸šçŸ¥è¯†ã€æ–°å…´æŠ€èƒ½ï¼‰
3. æ¯ä¸ªæŠ€èƒ½å—æ˜¾ç¤ºæŠ€èƒ½åç§°å’Œé‡è¦æ€§è¯„åˆ†
4. ä½¿ç”¨é¢œè‰²æ·±åº¦è¡¨ç¤ºæŠ€èƒ½éœ€æ±‚ç¨‹åº¦ï¼ˆæ·±è‰²=é«˜éœ€æ±‚ï¼Œæµ…è‰²=ä½éœ€æ±‚ï¼‰
5. ç‚¹å‡»æŠ€èƒ½å—æ˜¾ç¤ºè¯¦ç»†è¯´æ˜å’Œå­¦ä¹ å»ºè®®
6. å“åº”å¼è®¾è®¡ï¼Œæ”¯æŒç§»åŠ¨è®¾å¤‡
7. ç¾è§‚çš„ç°ä»£UIè®¾è®¡

è¯·åˆ›å»ºè¿™ä¸ªäº¤äº’å¼æŠ€èƒ½çƒ­ç‚¹å›¾Artifactã€‚"""

                async for chunk in self.chat_stream_unified(
                    artifacts_prompt,
                    context={
                        "task_type": "artifact_generation",
                        "artifact_type": "interactive_heatmap"
                    },
                    session_id=f"heatmap_artifact_{job_title.replace(' ', '_')}"
                ):
                    if chunk.get("type") == "text_delta":
                        result_content += "\n" + chunk.get("content", "")
                    elif chunk.get("type") == "content_block_start":
                        content_block = chunk.get("content_block", {})
                        if content_block.get("type") == "tool_use":
                            artifacts_generated = True
                            logger.info("ğŸ¨ å¤‡ç”¨Artifactsç”ŸæˆæˆåŠŸ")
            
            # ğŸ”¥ æ„å»ºå¢å¼ºçš„å¯è§†åŒ–æ•°æ®ç»“æ„
            visualization_data = {
                "chart_type": "interactive_heatmap",
                "title": f"{job_title} Skills Heatmap - Germany 2025",
                "artifact_generated": artifacts_generated,
                "categories": [
                    {
                        "name": "Technical Skills",
                        "color": "#FF6B6B",
                        "skills": self._extract_skills_from_content(result_content, "technical")
                    },
                    {
                        "name": "Soft Skills", 
                        "color": "#4ECDC4",
                        "skills": self._extract_skills_from_content(result_content, "soft")
                    },
                    {
                        "name": "Industry Knowledge",
                        "color": "#45B7D1", 
                        "skills": self._extract_skills_from_content(result_content, "industry")
                    },
                    {
                        "name": "Emerging Skills",
                        "color": "#FFA07A",
                        "skills": self._extract_skills_from_content(result_content, "emerging")
                    }
                ],
                "interactive_features": [
                    "Click-to-expand skill details",
                    "Hover effects and tooltips", 
                    "Responsive design",
                    "Color-coded skill importance",
                    "Learning recommendations"
                ]
            }
            
            # è¿”å›å®Œæ•´çš„æŠ€èƒ½çƒ­ç‚¹å›¾æ•°æ®ï¼ŒåŒ…å«Artifactsä¿¡æ¯
            return {
                "success": True,
                "job_title": job_title,
                "market": "Germany",
                "generated_at": "2025-01-22",
                "websearch_used": websearch_used,
                "artifacts_generated": artifacts_generated,
                "visualization_data": visualization_data,
                "analysis_text": result_content,
                "chart_ready": True,
                "interactive": artifacts_generated,
                "source": f"Claude 4 Sonnet + {'WebSearch + Artifacts' if websearch_used and artifacts_generated else 'WebSearch' if websearch_used else 'AI Analysis'}",
                "features": [
                    "Real-time market data via WebSearch" if websearch_used else "AI analysis",
                    "Interactive Artifacts visualization" if artifacts_generated else "Static visualization data",
                    "Deep thinking analysis",
                    "German market focus", 
                    "Multi-dimensional skill assessment",
                    "Click-to-expand interactions" if artifacts_generated else "JSON data structure"
                ],
                "artifacts_info": {
                    "enabled": artifacts_generated,
                    "type": "interactive_heatmap" if artifacts_generated else "data_only",
                    "compatible_with": "Claude 4 Artifacts system"
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æŠ€èƒ½çƒ­ç‚¹å›¾ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "job_title": job_title,
                "fallback_message": "æŠ€èƒ½çƒ­ç‚¹å›¾ç”Ÿæˆæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "artifacts_generated": False
            }
    
    def _extract_skills_from_content(self, content: str, skill_type: str) -> List[Dict[str, Any]]:
        """ä»åˆ†æå†…å®¹ä¸­æå–æŠ€èƒ½æ•°æ® / Extract skill data from analysis content"""
        # ç®€åŒ–çš„æŠ€èƒ½æå–é€»è¾‘ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥æ›´å¤æ‚
        default_skills = {
            "technical": [
                {"name": "Python", "score": 95, "demand": "Very High"},
                {"name": "JavaScript", "score": 85, "demand": "High"},
                {"name": "SQL", "score": 80, "demand": "High"},
                {"name": "Git", "score": 75, "demand": "Medium"}
            ],
            "soft": [
                {"name": "Communication", "score": 90, "demand": "Very High"},
                {"name": "Problem Solving", "score": 88, "demand": "Very High"},
                {"name": "Teamwork", "score": 85, "demand": "High"},
                {"name": "Adaptability", "score": 80, "demand": "High"}
            ],
            "industry": [
                {"name": "Agile/Scrum", "score": 85, "demand": "High"},
                {"name": "DevOps", "score": 80, "demand": "High"},
                {"name": "Cloud Platforms", "score": 88, "demand": "Very High"},
                {"name": "Cybersecurity", "score": 75, "demand": "Medium"}
            ],
            "emerging": [
                {"name": "AI/Machine Learning", "score": 92, "demand": "Very High"},
                {"name": "Blockchain", "score": 70, "demand": "Medium"},
                {"name": "IoT", "score": 75, "demand": "Medium"},
                {"name": "Low-Code/No-Code", "score": 80, "demand": "High"}
            ]
        }
        
        return default_skills.get(skill_type, [])

    async def get_german_job_market_insights(self, query: str) -> str:
        """ç®€åŒ–çš„å¾·å›½å°±ä¸šå¸‚åœºæ´å¯Ÿ / Simplified German job market insights"""
        try:
            prompt = f"Provide German job market insights for: {query}"
            
            result_content = ""
            async for chunk in self.chat_stream_unified(prompt):
                if chunk.get("type") == "text":
                    result_content += chunk.get("content", "")
            
            return result_content
            
        except Exception as e:
            logger.error(f"German job market insights failed: {e}")
            return f"æ— æ³•è·å–å¸‚åœºæ´å¯Ÿ: {str(e)}" 