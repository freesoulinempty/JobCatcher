"""
Claude 4 Sonnet AIæœåŠ¡ / Claude 4 Sonnet AI service
ä¼˜åŒ–ç‰ˆæœ¬ - ç¬¦åˆå®˜æ–¹æ–‡æ¡£æ ‡å‡† / Optimized version - compliant with official documentation standards
"""

from typing import List, Dict, Any, Optional, AsyncGenerator
from anthropic import AsyncAnthropic
from ..core.config import settings
from ..models.job import JobPosting, JobMatch
from ..core.logging import get_logger
import json
from datetime import datetime

logger = get_logger("JobCatcher.claude")


class TokenUsageTracker:
    """Tokenä½¿ç”¨é‡è·Ÿè¸ªå™¨ / Token usage tracker"""
    
    def __init__(self):
        self.daily_usage = {}
        # Claude 4 Sonnetå®šä»· (USD per million tokens) / Claude 4 Sonnet pricing
        self.pricing = {
            "claude-sonnet-4-20250514": {"input": 3.0, "output": 15.0},
            "claude-opus-4-20250514": {"input": 15.0, "output": 75.0}
        }
    
    def log_usage(self, model: str, input_tokens: int, output_tokens: int, 
                  cache_creation_tokens: int = 0, cache_read_tokens: int = 0, 
                  web_search_requests: int = 0, session_id: str = None):
        """è®°å½•tokenä½¿ç”¨é‡ / Log token usage"""
        today = datetime.now().strftime("%Y-%m-%d")
        
        if today not in self.daily_usage:
            self.daily_usage[today] = {
                "sessions": {},
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_cache_creation_tokens": 0,
                "total_cache_read_tokens": 0,
                "total_web_search_requests": 0,
                "total_cost": 0.0,
                "cache_savings": 0.0,
                "model_usage": {}
            }
        
        daily_data = self.daily_usage[today]
        
        # ä¼šè¯çº§åˆ«ç»Ÿè®¡ / Session-level statistics
        if session_id:
            if session_id not in daily_data["sessions"]:
                daily_data["sessions"][session_id] = {
                    "input_tokens": 0,
                    "output_tokens": 0,
                    "cache_creation_tokens": 0,
                    "cache_read_tokens": 0,
                    "web_search_requests": 0,
                    "cost": 0.0,
                    "requests": 0
                }
            
            session_data = daily_data["sessions"][session_id]
            session_data["input_tokens"] += input_tokens
            session_data["output_tokens"] += output_tokens
            session_data["cache_creation_tokens"] += cache_creation_tokens
            session_data["cache_read_tokens"] += cache_read_tokens
            session_data["web_search_requests"] += web_search_requests
            session_data["requests"] += 1
            
        # å…¨å±€ç»Ÿè®¡ / Global statistics
        daily_data["total_input_tokens"] += input_tokens
        daily_data["total_output_tokens"] += output_tokens
        daily_data["total_cache_creation_tokens"] += cache_creation_tokens
        daily_data["total_cache_read_tokens"] += cache_read_tokens
        daily_data["total_web_search_requests"] += web_search_requests
        
        # æˆæœ¬è®¡ç®— / Cost calculation
        if model in self.pricing:
            pricing = self.pricing[model]
            cost = (input_tokens * pricing["input"] + output_tokens * pricing["output"]) / 1_000_000
            daily_data["total_cost"] += cost
            
            # ç¼“å­˜èŠ‚çœè®¡ç®— / Cache savings calculation
            cache_savings = cache_read_tokens * pricing["input"] * 0.9 / 1_000_000  # 90% discount for cache reads
            daily_data["cache_savings"] += cache_savings
            
            if session_id:
                daily_data["sessions"][session_id]["cost"] += cost
        
        # æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡ / Model usage statistics
        if model not in daily_data["model_usage"]:
            daily_data["model_usage"][model] = {
                "requests": 0,
                "input_tokens": 0,
                "output_tokens": 0,
                "cost": 0.0
            }
        
        daily_data["model_usage"][model]["requests"] += 1
        daily_data["model_usage"][model]["input_tokens"] += input_tokens
        daily_data["model_usage"][model]["output_tokens"] += output_tokens
        if model in self.pricing:
            daily_data["model_usage"][model]["cost"] += cost
        
        logger.info(f"Token usage - Model: {model}, Input: {input_tokens}, Output: {output_tokens}, "
                   f"Cache Created: {cache_creation_tokens}, Cache Read: {cache_read_tokens}, "
                   f"Web Searches: {web_search_requests}, Cost: ${cost:.4f}, Savings: ${cache_savings:.4f}")

    def get_daily_summary(self, date: str = None) -> Dict[str, Any]:
        """è·å–æ¯æ—¥ä½¿ç”¨ç»Ÿè®¡ / Get daily usage summary"""
        if not date:
            date = datetime.now().strftime("%Y-%m-%d")
        
        if date not in self.daily_usage:
            return {
                "date": date,
                "total_tokens": 0,
                "total_cost": 0.0,
                "cache_savings": 0.0,
                "requests": 0,
                "cache_hit_rate": 0.0
            }
        
        data = self.daily_usage[date]
        total_tokens = data["total_input_tokens"] + data["total_output_tokens"]
        total_requests = sum(session["requests"] for session in data["sessions"].values())
        cache_hit_rate = data["total_cache_read_tokens"] / max(data["total_input_tokens"], 1)
        
        return {
            "date": date,
            "total_tokens": total_tokens,
            "input_tokens": data["total_input_tokens"],
            "output_tokens": data["total_output_tokens"],
            "total_cost": data["total_cost"],
            "cache_savings": data["cache_savings"],
            "requests": total_requests,
            "cache_hit_rate": cache_hit_rate,
            "web_search_requests": data["total_web_search_requests"],
            "sessions": len(data["sessions"]),
            "model_usage": data["model_usage"]
        }

    def check_budget_alert(self, daily_budget: float = 5.0) -> Dict[str, Any]:
        """æ£€æŸ¥é¢„ç®—è­¦å‘Š / Check budget alert"""
        today = datetime.now().strftime("%Y-%m-%d")
        current_usage = self.get_daily_summary(today)
        used_amount = current_usage["total_cost"]
        percentage = (used_amount / daily_budget) * 100
        
        if percentage >= 100:
            alert_level = "exceeded"
        elif percentage >= 80:
            alert_level = "high"
        elif percentage >= 60:
            alert_level = "medium"
        else:
            alert_level = "low"
        
        return {
            "alert_level": alert_level,
            "used_amount": used_amount,
            "budget": daily_budget,
            "percentage": percentage,
            "remaining": max(0, daily_budget - used_amount)
        }


class ClaudeService:
    """Claude 4 Sonnet AIæœåŠ¡ç±» - ä¼˜åŒ–ç‰ˆæœ¬ / Claude 4 Sonnet AI service - optimized version"""
    
    def __init__(self):
        """åˆå§‹åŒ–ClaudeæœåŠ¡ / Initialize Claude service"""
        self.client = AsyncAnthropic(api_key=settings.anthropic_api_key)
        self.model = settings.claude_model
        self.token_tracker = TokenUsageTracker()
        
        # Sessionç¼“å­˜ç®¡ç† - å…³é”®ä¼˜åŒ– / Session cache management - key optimization
        self.session_system_cache = {}
        
        # ğŸ”¥ æ–°å¢ï¼šæ¶ˆæ¯å†å²ç®¡ç† / NEW: Message history management
        self.session_message_history = {}
        
        # Tokené™åˆ¶é…ç½® / Token limit configuration
        self.max_tokens_limit = {
            "simple_response": 2500,
            "detailed_analysis": 4000,
            "complex_task": 6000
        }
        
        logger.info("Claude 4 service initialized - optimized version with prompt caching and context management")

    def _initialize_session_context(self, session_id: str) -> str:
        """
        åˆå§‹åŒ–ä¼šè¯ä¸Šä¸‹æ–‡ - åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åˆ›å»ºç¼“å­˜
        Initialize session context - create cache only on first call
        """
        if session_id in self.session_system_cache:
            logger.info(f"â™»ï¸ Using existing cached system context for session: {session_id}")
            return self.session_system_cache[session_id]
        
        # ğŸ”¥ ä¼˜åŒ–ï¼šç®€åŒ–ç³»ç»Ÿæç¤ºè¯ï¼Œæ»¡è¶³1024tokenç¼“å­˜è¦æ±‚ä½†é¿å…é‡å¤ / OPTIMIZED: Simplified system prompt for cache efficiency
        system_prompt = """You are JobCatcher, an advanced AI career assistant powered by Claude 4 Sonnet, specializing in the German job market. Your mission is to help professionals find opportunities and advance their careers in Germany.

**Core Identity & Capabilities:**
- German job market expert with deep knowledge across all industries and regions
- Career counselor providing personalized guidance for job search, applications, and professional development
- Multilingual assistant supporting Chinese, German, and English communication

**Specialized Knowledge:**
- German employment landscape, salary trends, and hiring patterns across 16 BundeslÃ¤nder
- CV/resume optimization following German standards (DIN 5008, Lebenslauf format)
- Cover letter guidance with proper German business language
- Interview preparation covering German corporate culture and expectations
- Skills development pathways including certifications and Weiterbildung programs
- Professional networking strategies for German business communities
- Immigration requirements and visa guidance for international professionals

**Advanced Tools & Intelligence:**
- Real-time web search for current job market data, company information, and industry trends
- Native PDF document processing for resume analysis and optimization
- Vector-based job matching system for personalized recommendations
- Skills analysis and career planning with market-aligned suggestions

**Communication Style:**
- Natural, conversational tone with cultural sensitivity to German business practices
- Practical, actionable advice with specific next steps and timelines
- Empathetic guidance understanding job search challenges and career transitions
- Context-aware responses building on conversation history and user preferences

**Response Framework:**
Provide immediately useful guidance while encouraging deeper engagement. Include specific examples, real-world scenarios, and concrete action items. Reference relevant German companies, institutions, and resources when appropriate. Focus on delivering value through expertise in German job market dynamics and professional development strategies.

Your goal is to be the definitive career assistant for anyone seeking to advance professionally in Germany, whether German nationals, EU citizens, or international professionals navigating the Deutsche job market."""
        
        # ç¼“å­˜ç³»ç»Ÿæç¤º / Cache system prompt
        self.session_system_cache[session_id] = system_prompt
        logger.info(f"ğŸ”¥ Created and cached system context for session: {session_id} (approx. {len(system_prompt.split())} words)")
        
        return system_prompt

    def _manage_session_history(self, session_id: str, new_message: str, assistant_response: str = None):
        """
        ğŸ”¥ å…³é”®æ–°åŠŸèƒ½ï¼šç®¡ç†ä¼šè¯æ¶ˆæ¯å†å² / KEY NEW FEATURE: Manage session message history
        """
        if session_id not in self.session_message_history:
            self.session_message_history[session_id] = []
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ / Add user message
        self.session_message_history[session_id].append({
            "role": "user",
            "content": new_message
        })
        
        # å¦‚æœæœ‰åŠ©æ‰‹å“åº”ï¼Œæ·»åŠ åŠ©æ‰‹æ¶ˆæ¯ / If assistant response available, add assistant message
        if assistant_response:
            # ğŸ”¥ ä¼˜åŒ–ï¼šæˆªæ–­è¿‡é•¿çš„åŠ©æ‰‹å“åº”ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯ / Optimize: truncate long assistant responses
            truncated_response = self._truncate_long_response(assistant_response)
            self.session_message_history[session_id].append({
                "role": "assistant", 
                "content": truncated_response
            })
        
        # ğŸ”¥ ä¼˜åŒ–ï¼šé™åˆ¶å†å²é•¿åº¦ï¼Œä¿æŒæœ€è¿‘12æ¡æ¶ˆæ¯ï¼ˆ6è½®å¯¹è¯ï¼‰ / Optimize: limit to 12 messages (6 conversation turns)
        if len(self.session_message_history[session_id]) > 12:
            self.session_message_history[session_id] = self.session_message_history[session_id][-12:]
        
        # ğŸ”¥ è®¡ç®—å½“å‰å†å²çš„tokenä¼°ç®— / Calculate estimated tokens for current history
        estimated_tokens = self._estimate_history_tokens(self.session_message_history[session_id])
        
        logger.info(f"ğŸ“ Updated message history for session {session_id}: {len(self.session_message_history[session_id])} messages, ~{estimated_tokens} tokens")

    def _truncate_long_response(self, response: str, max_length: int = 800) -> str:
        """
        ğŸ”¥ æ–°åŠŸèƒ½ï¼šæˆªæ–­è¿‡é•¿çš„åŠ©æ‰‹å“åº”ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯ / NEW: Truncate long assistant responses
        """
        if len(response) <= max_length:
            return response
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­ / Try to truncate at sentence boundaries
        sentences = response.split('ã€‚')
        truncated = ""
        for sentence in sentences:
            if len(truncated + sentence + "ã€‚") <= max_length:
                truncated += sentence + "ã€‚"
            else:
                break
        
        if truncated:
            return truncated + "\n\n[Content truncated for context efficiency]"
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å¥å­è¾¹ç•Œï¼Œç›´æ¥æˆªæ–­ / Direct truncation if no sentence boundary found
            return response[:max_length] + "...\n\n[Content truncated for context efficiency]"

    def _estimate_history_tokens(self, history: List[Dict[str, str]]) -> int:
        """
        ğŸ”¥ æ–°åŠŸèƒ½ï¼šä¼°ç®—æ¶ˆæ¯å†å²çš„tokenæ•°é‡ / NEW: Estimate token count for message history
        """
        total_chars = 0
        for message in history:
            total_chars += len(message.get('content', ''))
        
        # ç²—ç•¥ä¼°ç®—ï¼šè‹±æ–‡~4å­—ç¬¦/tokenï¼Œä¸­æ–‡~2å­—ç¬¦/tokenï¼Œå–å¹³å‡3å­—ç¬¦/token
        # Rough estimate: English ~4 chars/token, Chinese ~2 chars/token, average 3 chars/token
        estimated_tokens = total_chars // 3
        return estimated_tokens

    def _compress_message_history(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        ğŸ”¥ æ–°åŠŸèƒ½ï¼šå‹ç¼©æ¶ˆæ¯å†å²ï¼Œä¿ç•™æœ€é‡è¦çš„ä¿¡æ¯ / NEW: Compress message history
        """
        if len(messages) <= 4:
            return messages
        
        # ä¿ç•™ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯å’Œæœ€å2è½®å¯¹è¯ / Keep first user message and last 2 conversation turns
        compressed = []
        
        # æ·»åŠ ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼ˆé€šå¸¸æ˜¯å…³é”®èƒŒæ™¯ä¿¡æ¯ï¼‰ / Add first message (usually key background info)
        if messages:
            compressed.append(messages[0])
        
        # æ·»åŠ æœ€è¿‘çš„4æ¡æ¶ˆæ¯ï¼ˆ2è½®å¯¹è¯ï¼‰ / Add last 4 messages (2 conversation turns)
        if len(messages) > 4:
            compressed.extend(messages[-4:])
        else:
            compressed.extend(messages[1:])  # é™¤äº†ç¬¬ä¸€æ¡çš„å…¶ä»–æ‰€æœ‰æ¶ˆæ¯
        
        return compressed

    def _build_message_history(self, session_id: str, current_message: str, context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ğŸ”¥ å…³é”®æ–°åŠŸèƒ½ï¼šæ„å»ºå®Œæ•´çš„æ¶ˆæ¯å†å²ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ / KEY NEW FEATURE: Build complete message history with context
        """
        messages = []
        
        # è·å–å†å²æ¶ˆæ¯ / Get historical messages
        if session_id and session_id in self.session_message_history:
            messages.extend(self.session_message_history[session_id])
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„å½“å‰æ¶ˆæ¯ / CRITICAL FIX: Build current message with context
        message_content = []
        
        # å¦‚æœæœ‰æ–‡ä»¶ä¸Šä¼ ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­ / If file upload context exists, add to message
        if context and (context.get('resume_uploaded') or context.get('uploaded_file')):
            if context.get('uploaded_file'):
                # ğŸ”¥ æ–°åŠŸèƒ½ï¼šå¤„ç†Claude 4åŸç”Ÿæ–‡æ¡£æ ¼å¼ / NEW: Handle Claude 4 native document format
                uploaded_file = context['uploaded_file']
                document_data = uploaded_file.get('document_data', {})
                
                if document_data.get('claude_format') == 'native_pdf':
                    # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨Claude 4åŸç”ŸPDFæ”¯æŒ / CRITICAL: Use Claude 4 native PDF support
                    message_content.append({
                        "type": "document",
                        "source": document_data["source"]
                    })
                    
                    # æ·»åŠ æ–‡æœ¬è¯´æ˜ / Add text description
                    message_content.append({
                        "type": "text",
                        "text": f"""I have uploaded my resume file "{uploaded_file.get('filename', 'resume')}". Please analyze this PDF document directly using your native PDF processing capabilities.

{current_message}"""
                    })
                    
                    logger.info(f"ğŸ“„ Using Claude 4 native PDF processing for session {session_id}: {uploaded_file.get('filename', 'unknown')}")
                    
                elif document_data.get('claude_format') in ['text', 'extracted_text']:
                    # æ–‡æœ¬æ ¼å¼æ–‡æ¡£ / Text format document
                    file_content = document_data.get('content', '')
                    message_content.append({
                        "type": "text",
                        "text": f"""I have uploaded my resume file "{uploaded_file.get('filename', 'resume')}". Here is the content:

{file_content}

{current_message}"""
                    })
                    
                    logger.info(f"ğŸ“„ Using extracted text for session {session_id}: {uploaded_file.get('filename', 'unknown')}")
                    
                else:
                    # å›é€€åˆ°æ—§æ ¼å¼ / Fallback to old format
                    file_content = uploaded_file.get('text_content', '')
                    if file_content:
                        message_content.append({
                            "type": "text",
                            "text": f"""I have uploaded my resume file "{uploaded_file.get('filename', 'resume')}". Here is the content:

{file_content}

{current_message}"""
                        })
                        logger.info(f"ğŸ“„ Using fallback text content for session {session_id}: {uploaded_file.get('filename', 'unknown')}")
                    else:
                        # å®Œå…¨å›é€€ / Complete fallback
                        message_content.append({
                            "type": "text", 
                            "text": current_message
                        })
                        logger.warning(f"âš ï¸ No document content available for session {session_id}")
            
            elif context.get('file_content'):
                # å‘åå…¼å®¹æ—§æ ¼å¼ / Backward compatibility with old format
                filename = context.get('filename', 'resume')
                file_content = context.get('file_content', '')
                
                message_content.append({
                    "type": "text",
                    "text": f"""I have uploaded my resume file "{filename}". Here is the content:

{file_content}

{current_message}"""
                })
                
                logger.info(f"ğŸ“„ Using legacy file content format for session {session_id}: {filename}")
            
            else:
                # æ²¡æœ‰æ–‡ä»¶å†…å®¹ / No file content
                message_content.append({
                    "type": "text",
                    "text": current_message
                })
                logger.warning(f"âš ï¸ Resume upload context exists but no content found for session {session_id}")
        
        # å¦‚æœæœ‰èŒä½æ•°æ®ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­ / If job data context exists, add to message
        elif context and context.get('job_postings'):
            job_count = len(context['job_postings'])
            message_content.append({
                "type": "text",
                "text": f"""{current_message}

Context: I have access to {job_count} job postings from the German job market that can be used for matching and analysis."""
            })
            
            logger.info(f"ğŸ’¼ Enhanced message with job postings context for session {session_id}: {job_count} jobs")
        
        else:
            # æ™®é€šæ¶ˆæ¯ / Regular message
            message_content.append({
                "type": "text",
                "text": current_message
            })
        
        # æ·»åŠ æ„å»ºçš„æ¶ˆæ¯å†…å®¹ / Add built message content
        messages.append({
            "role": "user",
            "content": message_content
        })
        
        logger.info(f"ğŸ”— Built message history for session {session_id}: {len(messages)} total messages, content_blocks: {len(message_content)}, context_enhanced: {bool(context and (context.get('resume_uploaded') or context.get('job_postings')))}")
        return messages

    def _should_use_web_search(self, message: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦webæœç´¢ / Determine if web search is needed"""
        message_lower = message.lower()
        
        # æ—¶é—´æ€§å…³é”®è¯ / Time-related keywords
        time_keywords = ['2025', 'æœ€æ–°', 'å½“å‰', 'ç°åœ¨', 'current', 'latest', 'ä»Šå¹´', 'recent', 'now', 'æ–°çš„', 'new']
        # å¸‚åœºæ€§å…³é”®è¯ / Market-related keywords  
        market_keywords = ['è¶‹åŠ¿', 'è–ªèµ„', 'å·¥èµ„', 'salary', 'trend', 'market', 'å°±ä¸šå‰æ™¯', 'è¡Œä¸šå‘å±•', 'employment', 'å‰æ™¯', 'æ•°æ®', 'statistics', 'å¸‚åœºæ•°æ®', 'è¡Œæƒ…']
        # æ˜ç¡®æœç´¢è¯·æ±‚ / Explicit search requests
        search_keywords = ['æœç´¢', 'æŸ¥è¯¢æœ€æ–°', 'search for', 'find recent', 'lookup current', 'æœç´¢2025', 'æŸ¥æ‰¾æœ€æ–°']
        
        has_time = any(keyword in message_lower for keyword in time_keywords)
        has_market = any(keyword in message_lower for keyword in market_keywords)
        explicit_search = any(keyword in message_lower for keyword in search_keywords)
        
        # å¿…é¡»åŒæ—¶åŒ…å«æ—¶é—´æ€§å’Œå¸‚åœºæ€§å…³é”®è¯ï¼Œæˆ–è€…æ˜¯éå¸¸æ˜ç¡®çš„æœç´¢è¯·æ±‚ / Must have both time and market keywords, or very explicit search
        result = (has_time and has_market) or explicit_search
        
        logger.info(f"ğŸ” Web search decision for '{message[:50]}...': time={has_time}, market={has_market}, explicit={explicit_search}, result={result}")
        
        return result

    def _build_system_prompt_with_cache(self, session_id: str) -> List[Dict[str, Any]]:
        """æ„å»ºå¸¦ç¼“å­˜çš„ç³»ç»Ÿæç¤º / Build system prompt with cache"""
        
        system_content = self._initialize_session_context(session_id)
        
        return [
            {
                "type": "text", 
                "text": system_content,
                "cache_control": {"type": "ephemeral"}  # ç¼“å­˜æ§åˆ¶ / Cache control
            }
        ]

    def _build_tools_config(self, task_type: str, message: str) -> List[Dict[str, Any]]:
        """æ„å»ºå·¥å…·é…ç½® / Build tools configuration"""
        tools = []
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦webæœç´¢ / Check if web search is needed
        needs_web_search = self._should_use_web_search(message)
        
        if needs_web_search:
            tools.append({
                "type": "web_search_20250305",
                "name": "web_search",
                "max_uses": 3,
                "user_location": {
                    "type": "approximate", 
                    "country": "DE",
                    "timezone": "Europe/Berlin"
                }
            })
        
        return tools

    def _determine_task_type(self, message: str, context: Optional[Dict[str, Any]] = None) -> str:
        """ç¡®å®šä»»åŠ¡ç±»å‹ / Determine task type"""
        message_lower = message.lower()
        
        if any(keyword in message_lower for keyword in [
            "heatmap", "skills analysis", "market trends", "æŠ€èƒ½çƒ­åŠ›å›¾", "å¸‚åœºåˆ†æ", "comprehensive analysis"
        ]):
            return "complex_task"
        elif any(keyword in message_lower for keyword in [
            "analyze", "match", "resume", "cv", "è¯¦ç»†", "åˆ†æ", "åŒ¹é…", "ç®€å†", "job matching"
        ]):
            return "detailed_analysis"
        else:
            return "simple_response"
    
    async def chat_stream_unified(
        self, 
        message: str, 
        context: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Claude 4ç»Ÿä¸€æµå¼èŠå¤©æ¥å£ - ç¬¦åˆå®˜æ–¹æ–‡æ¡£çš„å®ç°ï¼Œæ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†
        Claude 4 unified streaming chat interface - official documentation compliant with context management
        """
        try:
            logger.info(f"Starting Claude 4 optimized chat for session: {session_id or 'new'}")
            
            # æ£€æŸ¥é¢„ç®— / Check budget
            budget_status = self.token_tracker.check_budget_alert()
            if budget_status["alert_level"] == "exceeded":
                yield {
                    "type": "error",
                    "content": f"Daily budget exceeded: ${budget_status['used_amount']:.2f}. Please try again tomorrow."
                }
                return

            # é…ç½®è¯·æ±‚å‚æ•° / Configure request parameters
            task_type = self._determine_task_type(message, context)
            max_tokens = self.max_tokens_limit.get(task_type, 2500)
            tools_config = self._build_tools_config(task_type, message)
            
            # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šæ„å»ºå®Œæ•´çš„æ¶ˆæ¯å†å²ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ / CRITICAL FIX: Build complete message history with context
            messages = self._build_message_history(session_id, message, context)
            
            # ğŸ”¥ Tokenç›‘æ§ï¼šæ£€æŸ¥è¾“å…¥tokenæ•°é‡ / Token monitoring: check input token count
            estimated_input_tokens = self._estimate_history_tokens(messages)
            if estimated_input_tokens > 3000:
                logger.warning(f"âš ï¸ High input token count for session {session_id}: ~{estimated_input_tokens} tokens")
                if estimated_input_tokens > 5000:
                    # å¦‚æœtokenè¿‡å¤šï¼Œè¿›è¡Œå‹ç¼© / Compress if too many tokens
                    messages = self._compress_message_history(messages)
                    logger.info(f"ğŸ—œï¸ Compressed message history to ~{self._estimate_history_tokens(messages)} tokens")
            
            # ğŸ”¥ æ ¸å¿ƒä¼˜åŒ–ï¼šæ ¹æ®å®˜æ–¹æ–‡æ¡£æ„å»ºAPIè¯·æ±‚ï¼ŒåŒ…å«æ¶ˆæ¯å†å² / Core optimization: build API request with message history
            request_config = {
                "model": self.model,
                "max_tokens": max_tokens,
                "messages": messages,  # ğŸ”¥ ä½¿ç”¨å®Œæ•´æ¶ˆæ¯å†å² / Use complete message history
            }
            
            # æ·»åŠ ç¼“å­˜çš„ç³»ç»Ÿæç¤º / Add cached system prompt
            if session_id:
                request_config["system"] = self._build_system_prompt_with_cache(session_id)
            
            # åªæœ‰éœ€è¦æ—¶æ‰æ·»åŠ å·¥å…· / Add tools only when needed
            if tools_config:
                request_config["tools"] = tools_config
            
            # æ—¥å¿—è®°å½• / Logging
            web_search_enabled = any(tool.get('type') == 'web_search_20250305' for tool in tools_config)
            
            logger.info(f"Claude 4 optimized request: {max_tokens} max_tokens, {len(tools_config)} tools, "
                       f"task_type: {task_type}, message: '{message[:50]}...', "
                       f"cached_system: {'Yes' if session_id else 'No'}, history_length: {len(messages)}")
            logger.info(f"ğŸ› ï¸ Tools enabled: {[t.get('name', t.get('type')) for t in tools_config]}, Web search: {'âœ…' if web_search_enabled else 'âŒ'}")
            
            # ğŸ”¥ ä¼˜åŒ–ï¼šç«‹å³å‘é€èŒä½æ•°æ®ï¼Œé¿å…ç”¨æˆ·ç­‰å¾… / Optimized: Send job data immediately to avoid waiting
            if context and context.get('job_postings'):
                jobs_data = []
                for job in context['job_postings']:
                    # ç¡®ä¿descriptionå­—æ®µå®Œæ•´ / Ensure description field is complete
                    description = ""
                    if hasattr(job, 'description'):
                        description = job.description or ""
                    else:
                        description = job.get('description', job.get('full_description', ''))
                    
                    jobs_data.append({
                        "id": job.get('id', ''),
                        "title": job.get('title', ''),
                        "company_name": job.get('company_name', ''),
                        "location": job.get('location', ''),
                        "description": description,
                        "url": job.get('url', ''),
                        "work_type": job.get('work_type', 'Full-time'),
                        "salary": job.get('salary', ''),
                        "source": job.get('source', 'LinkedIn'),
                        "posted_time_ago": job.get('posted_time_ago', '')
                    })
                
                yield {
                    "type": "job_data",
                    "jobs": jobs_data,
                    "match_type": context.get('match_type', 'general'),
                    "total_jobs": len(jobs_data),
                    "session_id": session_id
                }
                
                logger.info(f"ğŸ“‹ âš¡ ç«‹å³å‘é€ {len(jobs_data)} ä¸ªèŒä½åˆ°å‰ç«¯ (æ— éœ€ç­‰å¾…botå®Œæˆ) / Immediately sent {len(jobs_data)} jobs to frontend (no need to wait for bot)")
            
            # å¼€å§‹æµå¼å“åº” / Start streaming response
            assistant_response = ""  # ğŸ”¥ æ”¶é›†åŠ©æ‰‹å“åº”ç”¨äºå†å²è®°å½• / Collect assistant response for history
            
            async with self.client.messages.stream(**request_config) as stream:
                text_content = ""
                tool_uses = []
                
                yield {
                    "type": "start",
                    "session_id": session_id,
                    "model": self.model,
                    "task_type": task_type,
                    "tools_available": len(tools_config),
                    "cache_optimized": bool(session_id),
                    "context_messages": len(messages)
                }
                
                async for event in stream:
                    try:
                        # å¤„ç†æµå¼äº‹ä»¶ / Handle streaming events
                        if event.type == 'content_block_start':
                            # å®‰å…¨åœ°å¤„ç†content_blockå¯¹è±¡ / Safely handle content_block object
                            content_block = getattr(event, 'content_block', {})
                            if hasattr(content_block, '__dict__'):
                                # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸ / Convert to serializable dict
                                content_block_dict = {
                                    "type": getattr(content_block, 'type', ''),
                                    "id": getattr(content_block, 'id', ''),
                                    "name": getattr(content_block, 'name', ''),
                                    "input": getattr(content_block, 'input', {}) if hasattr(content_block, 'input') else {}
                                }
                            else:
                                content_block_dict = content_block
                                
                            yield {
                                "type": "content_block_start",
                                "index": getattr(event, 'index', 0),
                                "content_block": content_block_dict
                            }
                        
                        elif event.type == 'content_block_delta':
                            # å¤„ç†æ–‡æœ¬å¢é‡ / Handle text delta
                            if hasattr(event.delta, 'text') and event.delta.text:
                                text_chunk = event.delta.text
                                text_content += text_chunk
                                assistant_response += text_chunk  # ğŸ”¥ æ”¶é›†å“åº” / Collect response
                                yield {
                                    "type": "text_delta",
                                    "content": text_chunk
                                }
                        
                        elif event.type == 'content_block_stop':
                            yield {
                                "type": "content_block_stop",
                                "index": getattr(event, 'index', 0)
                            }
                        
                        elif event.type == 'message_stop':
                            # æµå¼ç»“æŸï¼Œä½¿ç”¨final_textå’Œusageä¿¡æ¯
                            yield {
                                "type": "message_stop"
                            }
                    
                    except Exception as event_error:
                        logger.error(f"Error processing stream event: {event_error}")
                        continue
                
                # ğŸ”¥ ä¼˜åŒ–ï¼šåœ¨æµå¼å“åº”å¼€å§‹æ—¶å°±å‘é€èŒä½æ•°æ®ï¼Œæ— éœ€ç­‰å¾…botå®Œæˆ / Optimized: Send job data at start of streaming
                # é¿å…ç”¨æˆ·ç­‰å¾…ï¼Œæå‡ç”¨æˆ·ä½“éªŒ / Avoid user waiting, improve UX
                
                # ğŸ”¥ å…³é”®ï¼šæ›´æ–°ä¼šè¯å†å² / CRITICAL: Update session history
                if session_id and assistant_response.strip():
                    self._manage_session_history(session_id, message, assistant_response.strip())
                
                # è·å–æœ€ç»ˆä½¿ç”¨ç»Ÿè®¡ / Get final usage statistics
                try:
                    final_message = await stream.get_final_message()
                    if hasattr(final_message, 'usage') and final_message.usage:
                        usage = final_message.usage
                        
                        # ğŸ”¥ å…³é”®ï¼šæ­£ç¡®è§£æå®˜æ–¹APIå“åº” / Key: correctly parse official API response
                        input_tokens = getattr(usage, 'input_tokens', 0)
                        output_tokens = getattr(usage, 'output_tokens', 0)
                        cache_creation_tokens = getattr(usage, 'cache_creation_input_tokens', 0)
                        cache_read_tokens = getattr(usage, 'cache_read_input_tokens', 0)
                        
                        # Webæœç´¢ä½¿ç”¨é‡ / Web search usage
                        web_search_requests = 0
                        if hasattr(usage, 'server_tool_use') and usage.server_tool_use:
                            web_search_requests = getattr(usage.server_tool_use, 'web_search_requests', 0)
                        
                        # è®°å½•tokenä½¿ç”¨ / Log token usage
                        self.token_tracker.log_usage(
                            model=self.model,
                            input_tokens=input_tokens,
                            output_tokens=output_tokens,
                            cache_creation_tokens=cache_creation_tokens,
                            cache_read_tokens=cache_read_tokens,
                            web_search_requests=web_search_requests,
                            session_id=session_id
                        )
                        
                        # è®°å½•é‡è¦äº‹ä»¶ / Log important events
                        if cache_read_tokens > 0:
                            logger.info(f"âœ… Claude 4 cache hit! Read {cache_read_tokens} tokens, saved ${cache_read_tokens * 3.0 * 0.9 / 1_000_000:.4f}")
                        if cache_creation_tokens > 0:
                            logger.info(f"ğŸ“ Claude 4 cache created! Wrote {cache_creation_tokens} tokens")
                        if web_search_requests > 0:
                            logger.info(f"ğŸ” Claude 4 web search used {web_search_requests} times")
                    else:
                        logger.warning("No usage information available from Claude 4 response")
                        
                except Exception as usage_error:
                    logger.error(f"Failed to get usage statistics: {usage_error}")

                # å®Œæˆå“åº” / Complete response
                yield {
                    "type": "complete",
                    "session_id": session_id,
                    "response_length": len(text_content),
                    "tools_used": len(tool_uses),
                    "cache_optimized": bool(session_id),
                    "context_preserved": bool(session_id and assistant_response.strip())
                }
                    
        except Exception as e:
            logger.error(f"Claude 4 streaming failed: {e}")
            yield {
                "type": "error",
                "content": f"AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚é”™è¯¯: {str(e)}"
            }

    # ğŸ”¥ æ–°å¢ï¼šæ¸…é™¤ä¼šè¯å†å²æ–¹æ³• / NEW: Clear session history method
    def clear_session_history(self, session_id: str):
        """æ¸…é™¤æŒ‡å®šä¼šè¯çš„å†å²è®°å½• / Clear history for specified session"""
        if session_id in self.session_message_history:
            del self.session_message_history[session_id]
            logger.info(f"ğŸ—‘ï¸ Cleared message history for session: {session_id}")
        
        if session_id in self.session_system_cache:
            del self.session_system_cache[session_id]
            logger.info(f"ğŸ—‘ï¸ Cleared system cache for session: {session_id}")

    # ğŸ”¥ æ–°å¢ï¼šè·å–ä¼šè¯å†å²æ–¹æ³• / NEW: Get session history method
    def get_session_history(self, session_id: str) -> List[Dict[str, str]]:
        """è·å–æŒ‡å®šä¼šè¯çš„å†å²è®°å½• / Get history for specified session"""
        return self.session_message_history.get(session_id, [])
    
    async def get_token_usage_stats(self, session_id: str = None) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ / Get usage statistics"""
        daily_stats = self.token_tracker.get_daily_summary()
        budget_status = self.token_tracker.check_budget_alert()
        
        # ç¡®ä¿åŒ…å«å½“å‰sessionçš„ç»Ÿè®¡æ•°æ® / Ensure current session stats are included
        if session_id and daily_stats.get('total_tokens', 0) == 0:
            # å¦‚æœæ²¡æœ‰ç»Ÿè®¡æ•°æ®ï¼Œå°è¯•ä»å†…å­˜ä¸­è·å– / If no stats, try to get from memory
            logger.info(f"No daily stats found, using session data for {session_id}")
        
        return {
            "daily_usage": daily_stats,
            "budget_status": budget_status,
            "optimization_benefits": {
                "daily_cache_savings": daily_stats.get("cache_savings", 0),
                "cache_hit_rate": daily_stats.get("cache_hit_rate", 0),
                "web_search_requests": daily_stats.get("web_search_requests", 0),
                "estimated_monthly_savings": daily_stats.get("cache_savings", 0) * 30
            },
            "claude4_features": {
                "prompt_caching": "1h TTL with 90% cost reduction",
                "web_search": "Native integration with geo-location",
                "streaming": "Real-time response delivery",
                "session_management": "Automatic context preservation",
                "message_history": f"Active sessions: {len(self.session_message_history)}"
            }
        }
    
    # ç®€åŒ–çš„å‘åå…¼å®¹æ–¹æ³• / Simplified backward compatibility methods
    async def analyze_resume(self, file_content: str, filename: str) -> Dict[str, Any]:
        """ç®€åŒ–çš„ç®€å†åˆ†æ / Simplified resume analysis"""
        try:
            prompt = f"Please analyze this resume file '{filename}' and provide structured analysis."
            
            result_content = ""
            async for chunk in self.chat_stream_unified(
                prompt, 
                context={"file_content": file_content, "filename": filename}
            ):
                if chunk.get("type") == "text":
                    result_content += chunk.get("content", "")
            
            return {
                "analysis_text": result_content,
                "skills": [],
                "experience_years": 0,
                "education_level": "Unknown",
                "languages": [],
                "summary": "Resume analyzed via Claude 4"
            }
            
        except Exception as e:
            logger.error(f"Resume analysis failed: {e}")
            return {"error": str(e), "summary": "Analysis failed"}

    async def match_jobs_with_resume(
        self, 
        resume_analysis: Dict[str, Any], 
        job_postings: List[JobPosting]
    ) -> List[JobMatch]:
        """
        ğŸ”¥ READMEæµç¨‹å®ç°ï¼šClaude 4æ·±åº¦åˆ†æç®€å†å’ŒèŒä½åŒ¹é… / README Flow: Claude 4 deep analysis for resume-job matching
        ä¸¥æ ¼æŒ‰ç…§READMEç¬¬5-6æ­¥ï¼šæ‹¼æ¥prompt â†’ Claudeåˆ†ææ’åº
        """
        try:
            # æ„å»ºä¸“é—¨çš„èŒä½æ¨èæç¤ºè¯ï¼ˆç§»å‡ºç³»ç»Ÿæç¤ºé¿å…é‡å¤ï¼‰/ Build dedicated job recommendation prompt
            job_matching_prompt = self._build_job_matching_prompt(resume_analysis, job_postings)
            
            # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨Claude 4è¿›è¡Œæ·±åº¦åˆ†æå’Œæ’åº / KEY: Use Claude 4 for deep analysis and ranking
            claude_analysis = ""
            async for chunk in self.chat_stream_unified(
                job_matching_prompt,
                context={
                    "task_type": "job_matching",
                    "resume_analysis": resume_analysis,
                    "job_count": len(job_postings)
                }
            ):
                if chunk.get("type") == "text":
                    claude_analysis += chunk.get("content", "")
            
            # è§£æClaude 4çš„åˆ†æç»“æœä¸ºJobMatchå¯¹è±¡ / Parse Claude 4 analysis into JobMatch objects
            job_matches = self._parse_claude_job_analysis(claude_analysis, job_postings)
            
            logger.info(f"âœ… Claude 4 job matching completed: {len(job_matches)} matches generated")
            return job_matches
            
        except Exception as e:
            logger.error(f"âŒ Job matching failed: {e}")
            return []

    def _build_job_matching_prompt(self, resume_analysis: Dict[str, Any], job_postings: List[JobPosting]) -> str:
        """
        ğŸ”¥ æ„å»ºèŒä½åŒ¹é…ä¸“ç”¨æç¤ºè¯ / Build dedicated job matching prompt
        æŒ‰ç…§READMEè¦æ±‚çš„åŒ¹é…è¦ç´ æƒé‡ï¼šæŠ€èƒ½>ç»éªŒ>åœ°ç‚¹>è¯­è¨€>å­¦å†
        """
        # æå–ç®€å†å…³é”®ä¿¡æ¯ / Extract key resume information
        skills = ', '.join(resume_analysis.get('skills', []))
        experience_years = resume_analysis.get('experience_years', 0)
        location = resume_analysis.get('location', 'æœªæŒ‡å®š')
        education = resume_analysis.get('education_level', 'æœªçŸ¥')
        languages = ', '.join(resume_analysis.get('languages', []))
        
        prompt = f"""ä½œä¸ºä¸“ä¸šçš„å¾·å›½å°±ä¸šå¸‚åœºé¡¾é—®ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ç®€å†åˆ†æ{len(job_postings)}ä¸ªèŒä½çš„åŒ¹é…åº¦å¹¶æ’åºã€‚

**ç®€å†æ¦‚è¦ï¼š**
- æŠ€èƒ½ï¼š{skills}
- å·¥ä½œç»éªŒï¼š{experience_years}å¹´
- æœŸæœ›åœ°ç‚¹ï¼š{location}
- æ•™è‚²èƒŒæ™¯ï¼š{education}
- è¯­è¨€èƒ½åŠ›ï¼š{languages}

**åŒ¹é…æƒé‡ï¼ˆé‡è¦æ€§æ’åºï¼‰ï¼š**
1. æŠ€èƒ½åŒ¹é…ï¼ˆ40%ï¼‰- é‡è¦
2. ç»éªŒåŒ¹é…ï¼ˆ30%ï¼‰- é‡è¦  
3. åœ°ç‚¹åŒ¹é…ï¼ˆ15%ï¼‰- ä¸­ç­‰é‡è¦
4. è¯­è¨€åŒ¹é…ï¼ˆ10%ï¼‰- ä¸­ç­‰é‡è¦
5. æ•™è‚²åŒ¹é…ï¼ˆ5%ï¼‰- è¾ƒä½é‡è¦

**å€™é€‰èŒä½ï¼š**
"""
        
        # æ·»åŠ æ‰€æœ‰èŒä½ä¿¡æ¯ / Add all job information
        for i, job in enumerate(job_postings, 1):
            job_desc = job.description[:300] if len(job.description) > 300 else job.description
            prompt += f"""
{i}. ã€{job.title}ã€‘- {job.company_name}
   åœ°ç‚¹ï¼š{job.location}
   å·¥ä½œç±»å‹ï¼š{getattr(job, 'job_type', 'æœªçŸ¥')}
   èŒä½æè¿°ï¼š{job_desc}...
"""

        prompt += """
**è¦æ±‚è¾“å‡ºJSONæ ¼å¼ï¼š**
è¯·ä¸ºæ¯ä¸ªèŒä½æä¾›è¯¦ç»†åˆ†æï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONç»“æ„è¿”å›ï¼š

```json
{
  "analysis_summary": "æ€»ä½“åˆ†ææ¦‚è¿°",
  "job_matches": [
    {
      "job_index": 1,
      "job_title": "èŒä½æ ‡é¢˜",
      "company_name": "å…¬å¸åç§°",
      "match_score": 85,
      "match_level": "excellent",
      "match_reasons": [
        "æŠ€èƒ½é«˜åº¦åŒ¹é…ï¼šPython, Reactç­‰æ ¸å¿ƒæŠ€èƒ½å®Œå…¨ç¬¦åˆ",
        "ç»éªŒå¹´é™åŒ¹é…ï¼šè¦æ±‚3-5å¹´ï¼Œå€™é€‰äººæœ‰4å¹´ç»éªŒ"
      ],
      "skill_matches": ["Python", "React", "Node.js"],
      "missing_skills": ["TypeScript", "Docker"],
      "location_match": true,
      "experience_match": true,
      "improvement_suggestions": [
        "å»ºè®®å­¦ä¹ TypeScriptæå‡å‰ç«¯å¼€å‘èƒ½åŠ›",
        "è¡¥å……Dockerå®¹å™¨åŒ–æŠ€èƒ½"
      ]
    }
  ]
}
```

**è¯„åˆ†æ ‡å‡†ï¼š**
- 90-100åˆ†ï¼šexcellentï¼ˆå®Œç¾åŒ¹é…ï¼‰
- 75-89åˆ†ï¼šgoodï¼ˆè‰¯å¥½åŒ¹é…ï¼‰
- 60-74åˆ†ï¼šfairï¼ˆä¸€èˆ¬åŒ¹é…ï¼‰
- 40-59åˆ†ï¼špoorï¼ˆè¾ƒä½åŒ¹é…ï¼‰
- 0-39åˆ†ï¼švery_poorï¼ˆä¸åŒ¹é…ï¼‰

è¯·æŒ‰åŒ¹é…åˆ†æ•°ä»é«˜åˆ°ä½æ’åºï¼Œæä¾›è¯¦ç»†çš„åŒ¹é…åŸå› å’Œæ”¹è¿›å»ºè®®ã€‚"""
        
        return prompt

    def _parse_claude_job_analysis(self, claude_analysis: str, job_postings: List[JobPosting]) -> List[JobMatch]:
        """
        ğŸ”¥ è§£æClaude 4çš„èŒä½åˆ†æç»“æœ / Parse Claude 4 job analysis results
        """
        import json
        import re
        
        try:
            # å°è¯•æå–JSONéƒ¨åˆ† / Try to extract JSON part
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', claude_analysis, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                analysis_data = json.loads(json_str)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•ç›´æ¥è§£æ / If no JSON format found, try direct parsing
                analysis_data = json.loads(claude_analysis)
            
            matches = []
            job_matches_data = analysis_data.get('job_matches', [])
            
            for match_data in job_matches_data:
                job_index = match_data.get('job_index', 1) - 1  # è½¬æ¢ä¸º0åŸºç´¢å¼•
                
                if 0 <= job_index < len(job_postings):
                    job = job_postings[job_index]
                    
                    # åˆ›å»ºJobMatchå¯¹è±¡ / Create JobMatch object
                    job_match = JobMatch(
                    job_posting=job,
                        matching_score=match_data.get('match_score', 50),
                        matching_reasons=match_data.get('match_reasons', []),
                        missing_skills=match_data.get('missing_skills', []),
                        recommended_improvements=match_data.get('improvement_suggestions', [])
                    )
                    
                    # æ·»åŠ é¢å¤–çš„åŒ¹é…ä¿¡æ¯ / Add additional matching info
                    job_match.skill_matches = match_data.get('skill_matches', [])
                    job_match.match_level = match_data.get('match_level', 'fair')
                    job_match.location_match = match_data.get('location_match', False)
                    job_match.experience_match = match_data.get('experience_match', False)
                    
                    matches.append(job_match)
            
            # æŒ‰åŒ¹é…åˆ†æ•°æ’åº / Sort by matching score
            matches.sort(key=lambda x: x.matching_score, reverse=True)
            
            logger.info(f"âœ… Parsed {len(matches)} job matches from Claude analysis")
            return matches
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Failed to parse Claude job analysis JSON: {e}")
            # åˆ›å»ºé»˜è®¤åŒ¹é…ç»“æœ / Create default match results
            return self._create_default_matches(job_postings)
        except Exception as e:
            logger.error(f"âŒ Error parsing Claude job analysis: {e}")
            return self._create_default_matches(job_postings)

    def _create_default_matches(self, job_postings: List[JobPosting]) -> List[JobMatch]:
        """åˆ›å»ºé»˜è®¤çš„èŒä½åŒ¹é…ç»“æœ / Create default job match results"""
        matches = []
        for i, job in enumerate(job_postings[:10]):  # é™åˆ¶å‰10ä¸ª
            matches.append(JobMatch(
                job_posting=job,
                matching_score=max(70 - i * 5, 40),  # é€’å‡åˆ†æ•°
                matching_reasons=["è‡ªåŠ¨åˆ†æ", "åŸºç¡€åŒ¹é…"],
                missing_skills=[],
                recommended_improvements=["è¯·ä¸Šä¼ è¯¦ç»†ç®€å†è·å¾—ç²¾å‡†åˆ†æ"]
            ))
        return matches

    async def generate_skill_heatmap_data(self, job_title: str) -> Dict[str, Any]:
        """ç®€åŒ–çš„æŠ€èƒ½çƒ­åŠ›å›¾ç”Ÿæˆ / Simplified skill heatmap generation"""
        try:
            prompt = f"Generate skill heatmap data for {job_title} positions in Germany 2025."
            
            result_content = ""
            async for chunk in self.chat_stream_unified(prompt):
                if chunk.get("type") == "text":
                    result_content += chunk.get("content", "")
            
            return {
                "heatmap_data": [],
                "analysis_text": result_content,
                "top_skills": ["Python", "AI/ML", "Cloud Computing"],
                "skill_categories": ["Technical", "Soft Skills", "Industry Knowledge"],
                "generated_by": "Claude 4 Sonnet"
            }
            
        except Exception as e:
            logger.error(f"Skill heatmap generation failed: {e}")
            return {"error": str(e)}

    async def get_german_job_market_insights(self, query: str) -> str:
        """ç®€åŒ–çš„å¾·å›½å°±ä¸šå¸‚åœºæ´å¯Ÿ / Simplified German job market insights"""
        try:
            prompt = f"Provide German job market insights for: {query}"
            
            result_content = ""
            async for chunk in self.chat_stream_unified(prompt):
                if chunk.get("type") == "text":
                    result_content += chunk.get("content", "")
            
            return result_content
            
        except Exception as e:
            logger.error(f"German job market insights failed: {e}")
            return f"æ— æ³•è·å–å¸‚åœºæ´å¯Ÿ: {str(e)}" 